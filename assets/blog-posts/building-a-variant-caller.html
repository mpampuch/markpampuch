<div class="blog-post">
  <div class="section-title">
    <p class="blog-post-title">
      Using Deep Learning to Build a Variant Caller for DNA Sequencing Data
    </p>
  </div>
  <p class="blog-post_paragraph">
    In 2022, One of the key pioneers of the CRIPSR gene editing technology and
    Gairdner Award recipients, Dr. Rodolphe Barrangou came to give a talk at the
    school I was pursuing my masters degree in. My lab was one of the few chosen
    to have a lunch with him. It was a very rare opportunity and I very
    fortunate to meet with such an accomplished researcher and hear some advice
    from one of the most successful scientists in the world. During the chat, he
    said something that stuck with me. "There is an everchanging adage in
    science. It used to be 'You'll never be at the forefront of research unless
    you understand statistics'. This then evolved into 'You'll never be at the
    forefront of research unless you understand programming'. The new one, and
    most important all now is 'You'll never be at the forefront of research
    unless you understand Deep Learning'. A human being can spend an entire
    lifetime analzying the sheer amount of data that can be collected in a
    single experiment, and so it's vital that you understand how to get a
    computer to help you make insights out of it".
  </p>
  <p class="blog-post_paragraph">
    I took this advice to heart and began learning about the field of machine
    learning and deep learning. I enrolled in a workshop titled "An Introduction
    to Machine Learning for Oxford Nanopore Data" hosted by the Oxford Nanopore
    AI team at the Oxford Nanopore Community meetup 2022. The workshop consisted
    of a brief introduction to deep learning models and then we were given three
    hours to try and build the most accurate the DNA variant caller we could. At
    the time, my knowledge of deep learning concepts and engineering tools was
    inadequate to complete the challenge, but I kept this problem in the back of
    my mind so that I could return to it when I had developped a better
    understanding of how deep learning works in order to cement knowledge on
    these important concepts in my field.
  </p>
  <p class="blog-post_paragraph">
    As a brief explaination of what a variant caller is, a
    <a
      href="https://www.ebi.ac.uk/training/online/courses/human-genetic-variation-introduction/variant-identification-and-analysis/"
      target="_blank"
      >variant caller</a
    >
    is a computational tool or algorithm used in the field of genomics to
    identify and characterize genetic variations, or variants, within an
    individual's DNA sequence. While each cell in our bodies has around six
    billion bases (letters) of DNA, most of these bases are identical in most
    humans. Therefore a sort of "unaninmous" DNA sequence can be built for human
    beings known as a reference sequence. The goal of variant calling is to
    figure out where in the DNA these bases differ in an individual compared to
    the reference sequence. These variants can include single nucleotide
    polymorphisms (SNPs), insertions, deletions, and structural variations.
  </p>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/01-seq-variants.png"
      alt="Sequence Variants"
    />
  </div>
  <p class="blog-post_paragraph">
    The process involves several steps, including alignment of raw DNA
    sequencing data to the reference sequence, identification of differences
    (variants) between the individual's sequence and the reference, and
    filtering to remove false positives. DNA variant callers use various
    statistical and computational methods to accurately identify true genetic
    variations while minimizing false positives and false negatives. They are
    crucial tools in genetics and genomics research, as well as in clinical
    settings for identifying genetic variants associated with diseases and other
    traits.
  </p>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/02-igv-alignment.png"
      alt="IGV Alignments"
    />
  </div>
  <p class="blog-post_paragraph">
    Real world variant callers very complex models built by teams of experts. In
    2018, Google built a state-of-the-art variant caller called
    <a href="https://www.nature.com/articles/nbt.4235" target="_blank"
      >DeepVariant</a
    >
    that achieves an overall variant calling accuracy of ~99.5%. This robust
    model takes in a plethora of input data (basecalled DNA sequencing reads,
    basecall accuracies, read mapping quality values, DNA strand information,
    variant-supporting read information, DNA base differences between the reads
    and reference genome, alternate allele information from parents or
    population, etc.).
  </p>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/03-deep-variant.png"
      alt="Deep Variant Architecture"
    />
  </div>
  <p class="blog-post_paragraph">
    Building such a model would take a team months of planning and validating
    and hence was the goal of the Oxford Nanopre machine learning workshop was
    not to generate a model that could simulate this state-of-the-art machine
    learning model but instead in order to build a model that can instead detect
    the presense of variants or mutations in simulated DNA sequencing data. The
    goal was also to develop a deep learning model that was robust enough to
    still detect variants and mutations in the raw DNA sequencing data when
    "noise" was added to the data through simulated sequencing error and
    alignment error. Once I became comfortable with the theory and practical
    implementations of machine learning and deep learning I attempted this
    project set out by the Oxford Nanopore Team. Below I've detailed the code on
    how I built and tested a deep learning model to tackle this challenge.
  </p>
  <p class="blog-post_paragraph">
    <strong> Note: </strong>
    The code shown here may be easier to read in notebook format. An interactive
    version of this blog-post can be found on Google Colab by

    <a
      href="https://colab.research.google.com/drive/1vl8pKKkiXnDc5nlB89vKH2PQyP7pxQA-?usp=sharing"
      target="_blank"
    >
      clicking here</a
    >.
  </p>
  <p class="blog-post_paragraph">
    <u><em>Step 1: Getting set up</em></u>
  </p>
  <p class="blog-post_paragraph">
    Below is some set up code I used to import the required libraries I'll need
    to take on this challenge in addition to set up code to format the outputs
    in the correct way
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Import all required libraries
import os
import shutil
import string
import re
import pathlib
from pathlib import Path
import requests
from zipfile import ZipFile
import torch
import torch.nn as nn
import torch.optim as optim
import torchvision
from torch.utils.data import DataLoader, TensorDataset
from torch.utils.tensorboard import SummaryWriter
from torchvision import datasets, transforms
import math
import random
import numpy as np
import pandas as pd    
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Dict, List, Tuple, Generator
import pickle
from IPython.display import Image
from PIL import Image as PILImage
from sklearn.model_selection import train_test_split
from tqdm.auto import tqdm
from timeit import default_timer as timer
torchinfo import summary
import torchmetrics, mlxtend
from torchmetrics import ConfusionMatrix
from mlxtend.plotting import plot_confusion_matrix

# Format plot output
%matplotlib inline
plt.rcParams.update({
"lines.color": "white",
"patch.edgecolor": "white",
"text.color": "white",
"axes.facecolor": "white",
"axes.edgecolor": "lightgray",
"axes.labelcolor": "white",
"xtick.color": "white",
"ytick.color": "white",
"grid.color": "lightgray",
"figure.facecolor": "black",
"figure.edgecolor": "black",
"figure.figsize": (7,4.5),
"figure.dpi" : 200,
"savefig.facecolor": "black",
"savefig.edgecolor": "black"})

# Set device agnostic code
device = "cuda" if torch.cuda.is_available() else "cpu"
</code></pre>
  <p class="blog-post_paragraph">
    <u><em>Step 2: Simulating alignment data</em></u>
  </p>
  <p class="blog-post_paragraph">
    The first step in any machine learning project is to get your data ready for
    use. Deep learning models work best when they have a large amount of data to
    train on. Therefore, it's important to have a way to simulate DNA alignments
    in order to give the models something to learn on. The Oxford Nanopore team
    has provided a function to simulate multiple sequence alignments.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Define the nucleotides
NUCLEOTIDES = "ACGT"

def simulate_alignments(reference_length: int = 200,
            num_alignments: int = 20000,
            depth: int = 100,
            p_sequencing_error: float = 0.0,
            p_alignment_error: float = 0.0,
            set_custom_mutation: Tuple[bool, int] = (False, _),
            set_rand_seed: Tuple[bool, int] = (False, _)
            ) -> Tuple[List[List[List[str]]], List[int]]:
  """
  Simulates alignments of DNA sequences.

  Parameters:
    reference_length (int): The length of the reference DNA sequence.
    num_alignments (int): The number of alignments to simulate.
    depth (int): The sequencing depth.
    p_sequencing_error (float): The probability of a sequencing error.
    p_alignment_error (float): The probability of an alignment error.
    set_custom_mutation (Tuple[bool, int]): A tuple indicating whether to use custom mutation types and the custom mutation type to use. 0 -> no SNP; 1 -> Homozygous SNP; 2 -> Heterozygous SNP
    set_rand_seed (Tuple[bool, int]): A tuple indicating whether to set the random seed and the seed value to use.

  Returns:
    Tuple[List[List[List[str]]], List[int]]: A tuple containing a list of alignments (each alignment is represented as a list of DNA sequences) and a list of mutation types for each alignment.
  Example:
    ([[['C', ..., 'G'], ... , ['C', ..., 'G']], ... , [['A', ... , 'T'], ... , ['A', ... , 'T']]], [2, ... , 1])
  """
  alignments = []
  mutation_types = []

  # Set random seed if specified
  if set_rand_seed[0]:
    random.seed(set_rand_seed[1])

  print("[INFO] Begin computing alignments...")

  # Iterate over the number of alignments to simulate
  for i in range(num_alignments):
    snp_index = reference_length // 2

    # Print progress message
    if (i + 1) % 400 == 0:
      print(f"[INFO] Computing alignment {i + 1}")

    # Generate reference sequence
    reference = [random.choice(NUCLEOTIDES) for _ in range(reference_length)]
    reference_base_at_snp = reference[snp_index]
    snp_base = random.choice([nuc for nuc in NUCLEOTIDES if nuc != reference_base_at_snp])

    # Determine mutation type
    if set_custom_mutation[0] and 0 <= set_custom_mutation[1] <= 2:
      mutation_type = set_custom_mutation[1]
    else:
      mutation_type = random.choice([0, 1, 2])  # 0 -> no SNP; 1 -> Homozygous SNP; 2 -> Heterozygous SNP
    mutation_types.append(mutation_type)

    # Initialize alignment with reference sequence
    alignment = [reference]

    # Generate reads
    for _ in range(depth):
      new_read = [reference[i] if random.random() > p_sequencing_error else random.choice(NUCLEOTIDES) for i in range(reference_length)]

      # Introduce alignment error
      if random.random() < p_alignment_error:
        snp_index += random.randint(-1, 2)

      # Introduce mutations
      if mutation_type == 1: # Homozygous SNP
        new_read[snp_index] = snp_base
      if mutation_type == 2 and random.random() > 0.5: # Heterozygous SNP
        new_read[snp_index] = snp_base

      # Add sequencing errors
      if random.random() < p_sequencing_error:
        new_read[snp_index] = random.choice(NUCLEOTIDES)

      alignment.append(new_read)

    # Append alignment to list
    alignments.append(alignment)

  # Print completion message and return results
  print(f"[INFO] Finished computing {num_alignments} alignment{'s' if num_alignments != 1 else ''}")
  return alignments, mutation_types</code></pre>
  <p class="blog-post_paragraph">
    This function generates a DNA sequence of length
    <code>reference_length</code> and generates a random reference. It then
    randomonly assigns a "mutation type" to the individual being sequenced:
    either that individual has no mutation, is heterozygous for a mutation
    (meaning they have a particular mutation on one of the two copies of their
    chromosomes), or is homozygous for a mutation (having a particular mutation
    in both copies of their chromosomes). This mutation is assigned in the
    middle of the DNA sequence. It then generates <code>depth</code> number of
    sequencing reads in the pattern of the individuals mutation signature and
    aligns them to reference sequence, simulating sequencing depth. It is able
    to simulate the probability of an incorrect DNA base being called by the
    sequencer and the probabilty of a read being mis-aligned to the reference
    sequence through the <code>p_sequencing_error</code> and
    <code>p_alignment_error</code> parameters, respectfully. It then repeats
    this whole process <code>num_alignments</code> times, generating multiple
    different sets of alignments at once and returning lists of the alignments
    and their corresponding label types.
  </p>
  <p class="blog-post_paragraph">
    To demonstrate, here are 3 randomly simulated sets of alignments without any
    sequencing or alignment errors.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Compute 3 sets of alignments without errors
RANDOM_SEED = 70
alignments, mutation_types = simulate_alignments(reference_length = 200,
                                                num_alignments = 3,
                                                depth = 100,
                                                p_sequencing_error = 0.0,
                                                p_alignment_error = 0.0,
                                                set_rand_seed = (True, RANDOM_SEED))
alignments = np.array(alignments)</code></pre>
  <p class="blog-post_paragraph">
    Let's visualize these alignments to see how each mutation type looks like in
    the DNA sequencing read data. For each of these sets of alignments, the
    first sequence (at a DNA sequencing depth of 0) represents the reference
    sequence. The rest of the data represent alignments of all the rest of the
    individual's sequencing reads to the reference sequence.
  </p>
  <p class="blog-post_paragraph">
    This is a diagram of how the aligned data will look.
  </p>
  <pre class="line-numbers"><code class="language-python"># Create dummy data
x = np.linspace(0, 200, 200) # 200 points on x-axis
y = np.linspace(0, 100, 100) # 100 points on y-axis

# Create a meshgrid for labeling purposes
X, Y = np.meshgrid(x, y)
X[0] = X[0] * 0
X[0][len(X[0]) // 2] = 2
X[1:] = X[1:] * 0 + 1

plt.imshow(X, cmap='bwr', aspect='auto')
plt.xlabel('Position along DNA Sequence')
plt.ylabel('Sequencing Depth')
plt.title('DNA Alignment Diagram')

plt.annotate('Reference Genome', xy=(len(x) // 30, 0.1), xytext=(len(x) // 30, 9),
              arrowprops=dict(facecolor='blue', shrink=0.05),
              color='blue')

plt.annotate('Locus of potential mutation', xy=(len(x) // 2, 0.1), xytext=(len(x) // 2 , 9),
              arrowprops=dict(facecolor='red', shrink=0.05),
              color='red')
# Add arrow annotation
plt.annotate('', xy=(90, 0), xytext=(90, 100),
              arrowprops=dict(arrowstyle='<->', color='black'))
# Add text annotation
plt.text(90 + 2, 50, 'Every row after the first one is \nan individual DNA sequencing \nread aligned to the reference',
          color='black', fontsize=11)
# Add horizontal lines every 1 unit on the y-axis
for i in range(1, 100):
    plt.hlines(i, 0, 199, colors='black', linewidth=0.1)

plt.show()</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/04-alignment-diagram.png"
      alt="Alignment Diagram"
    />
  </div>
  <p class="blog-post_paragraph">Now the actual data.</p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Prepare alignment data for plotting
transdict = {"A":0, "C": 1, "G":2, "T":3}
mutation_type_names = {0: "No mutation",
                  1: "Homozygous Mutation",
                  2: "Heterozygous Mutation"}
alignments_ints = np.vectorize(transdict.get)(alignments) # Convert the DNA alignments to numerical encodings</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python"># Plot first set of alignments
alignment_idx = 0
plt.imshow(alignments_ints[alignment_idx], cmap='jet')
plt.xlabel('Position along DNA Sequence')
plt.ylabel('Sequencing Depth')
plt.title('Mutation type - ' + mutation_type_names[mutation_types[alignment_idx]])
plt.show()</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/05-no-mutation.png"
      alt="Example of DNA sequencing alignments for an individual with no mutation"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Plot second set of alignments
alignment_idx = 1
plt.imshow(alignments_ints[alignment_idx], cmap='jet')
plt.xlabel('Position along DNA Sequence')
plt.ylabel('Sequencing Depth')
plt.title('Mutation type - ' + mutation_type_names[mutation_types[alignment_idx]])
plt.show()</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/06-heterozygous-mutation.png"
      alt="Example of DNA sequencing alignments for an individual with a heterozygous mutation"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Plot third set of alignments
alignment_idx = 2
plt.imshow(alignments_ints[alignment_idx], cmap='jet')
plt.xlabel('Position along DNA Sequence')
plt.ylabel('Sequencing Depth')
plt.title('Mutation type - ' + mutation_type_names[mutation_types[alignment_idx]])
plt.show()</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/07-homozygous-mutation.png"
      alt="Example of DNA sequencing alignments for an individual with a homozygous mutation"
    />
  </div>
  <p class="blog-post_paragraph">
    These alignments appear as expected. The alignment simulator function placed
    created no mutations, or created a mutation in one or both copies of an
    individuals chromosome halfway along the length of the reference sequence,
    in this case at position 100 along the DNA sequence. Looking at the
    alignments, you can see that:
  </p>
  <ul>
    <li>
      The DNA sequencing reads from the individual with no mutation look exactly
      like the reference sequence.
    </li>
    <li>
      Half the reads from the individual heterozygous for the mutuaton share the
      same DNA base (same colour) as the base in the reference sequence, while
      the other half of the reads differ.
    </li>
    <li>
      The reads from the individual homozygous for the mutuaton all differ from
      reference sequence.
    </li>
  </ul>

  <p class="blog-post_paragraph">
    One thing to notice already is that although it's fairly easy to spot the
    mutation signature of the heterozygous individual, its much harder to
    visually distinguish between an individual with no mutations and a
    homozygous individual with two mutations. When plotted out like this there's
    only about a 1 pixel difference between the two. This is something to keep
    in mind when trying to build a mutation classifier.
  </p>
  <p class="blog-post_paragraph">
    Now let's take a look at a more realistic scenario. When performing variant
    calling, there are two main areas where inconsitencies between an
    individual's DNA sequencing reads and a reference sequence can exist that
    don't come from a real biological mutation.
  </p>
  <ul>
    <li>
      <strong>Sequencing error</strong>: DNA is read by DNA sequencers. There
      are many different devices with different strategies for performing this
      task, but each of them come with their own pros and cons. Each of them
      have some amount of instrumental or chemical errors that can cause them to
      "mis-read" DNA bases and introduce errors into the raw sequencing data.
    </li>
    <li>
      <strong>Alignment error</strong>: In order for DNA sequencing reads to be
      compared to a reference sequence, they have to be aligned so that the
      sequences that are most similar to each other are stacked together. This
      is performed by computer algorithms often in a dynamic programming
      approach, which means the algorithm is trying to simplify a complicated
      problem by breaking it down into simpler sub-problems in a recursive
      manner. The output the algorithm produces is just the optimal way to solve
      all the little sub-problems, but may not necessarily be the optimial way
      to solve the overall problem. This approach is usually employed if the
      trying to solve the overall problem is prohibitive to do computationally.
      Different computer algorithms and different algorithm parameters can
      sometimes decide to align DNA sequences in differing ways. Additionally,
      if two (or more) sequences of the reference DNA are very similar,
      sometimes the alignment algorithm doesn't know which one to pick and can
      allocate reads that came from both of those regions to only one of those
      repetitive regions in the reference sequence if it has a bias for that
      region for whatever reason.
    </li>
  </ul>
  <p class="blog-post_paragraph">
    Let's simulate some sets of alignments with errors now. While sequencing
    error and alignment error are often very small, often less than 1% for each
    nowadays, I'll simulate them with higher percentages just to better
    visualize the effect these both have on the raw data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Compute a set of alignments with errors
sequencing_error = 0.15
alignment_error = 0.05

alignments, mutation_types = simulate_alignments(reference_length=200,
                                                num_alignments=10,
                                                depth=100,
                                                p_sequencing_error=sequencing_error,
                                                p_alignment_error=alignment_error,
                                                set_rand_seed=(True, RANDOM_SEED))
alignments = np.array(alignments)

alignments_ints = np.vectorize(transdict.get)(alignments) # Convert the DNA alignments to numerical encodings</code></pre>
  <p class="blog-post_paragraph">Now let's visualize the alignments.</p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Plot the set of alignments
def plot_alignment(alignment_idx):
    plt.imshow(alignments_ints[alignment_idx], cmap='jet')
    plt.xlabel('Position along DNA Sequence')
    plt.ylabel('Sequencing Depth')
    plt.title('Mutation type - ' + mutation_type_names[mutation_types[alignment_idx]], y=1.05)
    plt.text(0.5, 1.02, f'With {int(sequencing_error * 100)}% Sequencing error and {int(alignment_error * 100)}% Alignment error', horizontalalignment='center', fontsize=9, transform=plt.gca().transAxes)
    plt.show()

# Plot no mutation sample
plot_alignment(alignment_idx = 0)

# Plot heterozygous sample
plot_alignment(alignment_idx = 5)

# Plot homozygous sample
plot_alignment(alignment_idx = 2)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/08-no-mutation-errored.png"
      alt="Example of DNA sequencing alignments for an individual with no mutation and errors"
    />
  </div>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/09-heterozygous-mutation-errored.png"
      alt="Example of DNA sequencing alignments for an individual with a heterozygous mutation and errors"
    />
  </div>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/10-homozygous-mutation-errored.png"
      alt="Example of DNA sequencing alignments for an individual with a homozygous mutation and errors"
    />
  </div>
  <p class="blog-post_paragraph">
    As you can see, with more realistic data, it becomes much harder to see what
    kind of mutation profile this individual has.
  </p>
  <p class="blog-post_paragraph">
    If DNA sequencing and alignment technologies were perfect, there would be no
    need for variant callers. Any differences between the reference sequence and
    an individuals sequenced DNA would indicate the presence of DNA mutations.
    However these technologies are not perfect. Despite having small error
    rates, when dealing with DNA you're often dealing with reading numbers of
    bases into the billions, trillions, and beyond. These small error rates add
    up and cause the raw data to be very messy. The goal a variant caller is to
    isolate the real biological signal from all the "noise" in the data
    generated by the experiments.
  </p>
  <p class="blog-post_paragraph">
    In the following code I'll describe the steps I took to build and train a
    variant caller to classify the mutation profile of each individual. The
    goals of this project were four as four-fold:
  </p>
  <ol>
    <li>
      To create a neural network to classify indivuals as homozygous,
      heterozygous or no mutation based on data in the absence of errors
    </li>
    <li>To measure the model's robustness to sequencing error</li>
    <li>To measure its robustness to alignment error</li>
    <li>
      To assess how well a model trained on one error rate performs on another
      and to think about how this model can be made more robust for the "real
      world"
    </li>
  </ol>
  <p class="blog-post_paragraph">
    <u><em>Step 3: Setting up my data</em></u>
  </p>
  <p class="blog-post_paragraph">
    Preparing the data is a crucial initial step in any deep learning project.
    The quality and organization of the data can significantly impact the
    performance and reliability of the models. By structuring the data in a
    systematic way, we can streamline the training process and ensure accurate
    evaluation of the models.
  </p>
  <p class="blog-post_paragraph">
    I'm going to set up my alignment data as images (as plotted above). This is
    because my strategy will be to design convolutional neural networks to try
    and classify the mutation types. Convolutional neural networks (or CNN's)
    are a type of deep learning architecture particularly well-suited for
    image-related tasks. They can serve as powerful feature extractors and can
    be fine-tuned on smaller, domain-specific datasets, enabling rapid
    development and deployment of image-based applications even with limited
    data. The idea here is that if I can develop a robust CNN, it will be able
    to pick out the features in the alignment images that will reveal what
    mutation signatures the individuals possess. This is also inspired by what
    the Google team did with their DeepVariant model, where the variant calling
    data was encoded and piled-up into an image format in order to train a
    CNN-based model.
  </p>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/11-pileup.png"
      alt="Example of image encoding in Deep Variant"
    />
  </div>
  <p class="blog-post_paragraph">
    The goal here is to programmatically set up my data in the following
    directory structure.
  </p>
  <pre class="no-lang-display"><code class="language-output">data
├── train
│   ├── 0-percent-seq-error_0-percent-alignment-error
│   │   ├── heterozygous-mutation
│   │   │   ├── 00001.png
│   │   │   ├── 00002.png
|   |   |   ...
│   │   │   ├── 07999.png
│   │   │   └── 08000.png
│   │   ├── homozygous-mutation
│   │   │   ├── 00001.png
│   │   │   ├── 00002.png
|   |   |   ...
│   │   │   ├── 07999.png
│   │   │   └── 08000.png
│   │   └── no-mutation
│   │       ├── 00001.png
│   │       ├── 00002.png
|   |       ...
│   │       ├── 07999.png
│   │       └── 08000.png
│   └── X-percent-seq-error_Y-percent-alignment-error
│       ├── heterozygous-mutation
│       │   ├── 00001.png
│       │   ├── 00002.png
|       |   ...
│       │   ├── 07999.png
│       │   └── 08000.png
│       ├── homozygous-mutation
│       │   ├── 00001.png
│       │   ├── 00002.png
|       |   ...
│       │   ├── 07999.png
│       │   └── 08000.png
│       └── no-mutation
│           ├── 00001.png
│           ├── 00002.png
|           ...
│           ├── 07999.png
│           └── 08000.png
└── test
    ├── 0-percent-seq-error_0-percent-alignment-error
    │   ├── heterozygous-mutation
    │   │   ├── 08001.png
    │   │   ├── 08002.png
    |   |   ...
    │   │   ├── 09999.png
    │   │   └── 10000.png
    │   ├── homozygous-mutation
    │   │   ├── 08001.png
    │   │   ├── 08002.png
    |   |   ...
    │   │   ├── 09999.png
    │   │   └── 10000.png
    │   └── no-mutation
    │       ├── 08001.png
    │       ├── 08002.png
    |       ...
    │       ├── 09999.png
    │       └── 10000.png
    └── X-percent-seq-error_Y-percent-alignment-error
        ├── heterozygous-mutation
        │   ├── 08001.png
        │   ├── 08002.png
        |   ...
        │   ├── 09999.png
        │   └── 10000.png
        ├── homozygous-mutation
        │   ├── 08001.png
        │   ├── 08002.png
        |   ...
        │   ├── 09999.png
        │   └── 10000.png
        └── no-mutation
            ├── 08001.png
            ├── 08002.png
            ...
            ├── 09999.png
            └── 10000.png</code></pre>
  <p class="blog-post_paragraph">
    This kind of structure is useful for many reasons
  </p>
  <ul>
    <li>
      The data is automatically organized into training data and testing data.
      This division is vital for training robust models and accurately assessing
      their performance.
    </li>
    <li>
      It will be easy to keep track of data with different sequencing and
      alignment error.
    </li>
    <li>
      The names of the parent directories containing the images can be utilized
      to assign correct labels to each image. Having a large dataset of
      accurately labeled data is essential for any supervised learning task.
      Consistent and reliable labeling enables the model to learn meaningful
      patterns and make accurate predictions.
    </li>
  </ul>
  <p class="blog-post_paragraph">
    By structuring the data in this manner, it lays a solid foundation for
    subsequent steps in the workflow for developing deep learning models. An
    organized and systematic approach enhances reproducibility, scalability, and
    overall effectiveness of the project.
  </p>
  <p class="blog-post_paragraph">
    The next goal is to develop a function that can generate these files and
    directories when simulating the alignments.
  </p>
  <p class="blog-post_paragraph">
    Firstly I'll create a helper functions to help save the images in high
    quality format.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def save_alignment_image(alignment: List[List[int]],
                         save_filepath: str,
                         resolution: int = 200,
                         background_col: str = "white"
                         ) -> None:
  """
  Save the alignment data as an image.

  Parameters:
    alignment (List[List[int]]): The alignment data to be saved as an image.
    save_filepath (str): The file path to save the image.
    resolution (int): The resolution of the saved image. Default is 200 dpi.
    background_col (str): The background color of the image. Default is 'white'.

  Returns:
    None
  """

  # Set the face color of the figure
  plt.gcf().set_facecolor('white')

  # Plot the image
  plt.imshow(alignment, cmap='jet')
  plt.axis('off')  # Hide axis

  # Save it to .png and at a high dpi to preserve image quality if a filename is provided
  plt.savefig(save_filepath, dpi=resolution, bbox_inches='tight', facecolor=background_col)
  plt.close()


def save_alignment_data(X_data: List[List[List[int]]],
                        y_data: List[int],
                        data_path: pathlib.PosixPath,
                        mutation_path_dict: Dict[int, str],
                        add_rand_string_to_filename: bool = False):
  """
  Save alignment data as images.

  Parameters:
    X_data (List[List[List[int]]]): List of alignment data.
    y_data (List[int]): List of mutation types.
    data_path (pathlib.PosixPath): Path to save the data.
    mutation_path_dict (Dict[int, str]): Dictionary mapping mutation types to directory names.
    add_rand_string_to_filename (bool): Whether to add a random string to the filename. Default is False.

  Returns:
    None
  """

  for i, (alignment, mutation_type) in tqdm(enumerate(zip(X_data, y_data), start=1), total=len(X_data)):
    # Create the mutation directory
    mutation_dir = data_path / mutation_path_dict[mutation_type]
    mutation_dir.mkdir(parents=True, exist_ok=True)

    # Pad the filename with leading zeros based on the range and add random signature to each file
    buffer = 0
    rand_string = ""
    if add_rand_string_to_filename:
      rand_string = '_' + ''.join(random.choices(string.ascii_uppercase + string.digits, k = 5))
    filename = str(i + buffer).zfill(len(str(len(X_data)))) + rand_string + ".png"
    file_path = mutation_dir / filename

    # save the alignment to the directory
    save_alignment_image(alignment=alignment, save_filepath=file_path)
  print(f"[INFO] Data saved to {data_path}")</code></pre>
  <p class="blog-post_paragraph">
    Now I'll create a function to generate the data images and sort them into
    folders.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def create_dir_structure_for_alignments(p_train_data: float = 0.8,
    set_dev_set: Tuple[bool, float, str] = (False, _, _),
    set_rand_seed_data_split: Tuple[bool, int] = (False, _),
    remove_all_existing_data: bool = False,
    img_resolution: int = 200,
    img_background_col: str = "white",
    reference_length: int = 200,
    num_alignments: int = 20000,
    depth: int = 100,
    p_sequencing_error: float = 0.0,
    p_alignment_error: float = 0.0,
    set_custom_mutation: Tuple[bool, int] = (False, _),
    set_rand_seed_alignments: Tuple[bool, int] = (False, _),
    add_rand_string_to_filename: bool = False,
    ) -> None:
  """
  Create directory structure for storing alignment data and save the data.

  Parameters:
  p_train_data (float): Percentage of data to be used for training. Default is 0.8.
  set_dev_set (tuple): Tuple containing a boolean indicating whether to set a development set,
  the desired percentage split of the development set, and the method of splitting the data.
  Default is (False, _, _).
  set_rand_seed_data_split (tuple): Tuple containing a boolean indicating whether to set a random seed
  for data splitting and the random seed value. Default is (False, _).
  remove_all_existing_data (bool): Whether to remove all existing data in the directory. Default is False.
  img_resolution (int): Resolution of the images. Default is 200.
  img_background_col (str): Background color of the images. Default is "white".
  reference_length (int): Length of the reference sequence. Default is 200.
  num_alignments (int): Number of alignments to simulate. Default is 20000.
  depth (int): Depth of the alignment. Default is 100.
  p_sequencing_error (float): Probability of sequencing error. Default is 0.0.
  p_alignment_error (float): Probability of alignment error. Default is 0.0.
  set_custom_mutation (tuple): Tuple containing a boolean indicating whether to set a custom mutation
  and the custom mutation value. Default is (False, _).
  set_rand_seed_alignments (tuple): Tuple containing a boolean indicating whether to set a random seed
  for alignment generation and the random seed value. Default is (False, _).
  add_rand_string_to_filename (bool): Whether to add a random string to the filename. Default is False.
  """
  # Perform alignments
  alignments, mutation_types = simulate_alignments(reference_length=reference_length,
              num_alignments=num_alignments,
              depth=depth,
              p_sequencing_error=p_sequencing_error,
              p_alignment_error=p_alignment_error,
              set_custom_mutation=set_custom_mutation,
              set_rand_seed=set_rand_seed_alignments)

  alignments = np.array(alignments)
  mutation_types = np.array(mutation_types)

  alignments_ints = np.vectorize(transdict.get)(alignments) # Convert the DNA alignments to numerical encodings

  # Create train/test/split
  random_seed = set_rand_seed_data_split[1] if set_rand_seed_data_split[0] else None
  X_train, X_test, y_train, y_test = train_test_split(alignments_ints, mutation_types, train_size=0.8, random_state=random_seed)

  # Create dev set if specified
  X_dev, y_dev = (np.empty(0), np.empty(0))
  if set_dev_set[0]:
  desired_p_dev_set_split = set_dev_set[1]
  # Create dev set by splitting up the train set
  if set_dev_set[2] == "split_train" and set_dev_set[1] <= p_train_data: # Only split train set if mathematically possible
  actual_p_dev_set_split = round((1 / (p_train_data)) * (desired_p_dev_set_split), 4)
  X_dev, X_train, y_dev, y_train = train_test_split(X_train, y_train, train_size=actual_p_dev_set_split, random_state=random_seed)
  # Create dev set by splitting up the test set (default)
  elif set_dev_set[1] <= (1 - p_train_data): # Only split test set if mathematically possible
  actual_p_dev_set_split = round((1 / (1 - p_train_data)) * (desired_p_dev_set_split), 4)
  X_dev, X_test, y_dev, y_test = train_test_split(X_test, y_test, train_size=actual_p_dev_set_split, random_state=random_seed)


  # Set up directory structure
  data_path = Path("data/")
  error_path = f"{int(p_sequencing_error * 100)}-percent-seq-error_{int(p_alignment_error * 100)}-percent-alignment-error"
  mutation_path_dict = {0: "no-mutation", 1: "homozygous-mutation", 2: "heterozygous-mutation"}

  # Remove existing data if set to true
  if remove_all_existing_data:
  if data_path.exists():
  print(f"[INFO] Removing previous data from {data_path}")
  shutil.rmtree(data_path)

  # Create train directory
  train_path = data_path / "train" / error_path
  train_path.mkdir(parents=True, exist_ok=True)

  # Create test directory
  test_path = data_path / "test" / error_path
  test_path.mkdir(parents=True, exist_ok=True)

  # Save training data
  print("[INFO] Saving training data...")
  save_alignment_data(X_train, y_train, train_path, mutation_path_dict, add_rand_string_to_filename)

  # Save testing data
  print("[INFO] Saving testing data...")
  save_alignment_data(X_test, y_test, test_path, mutation_path_dict, add_rand_string_to_filename)

  # Create dev directory if dev set exists
  if (X_dev.size != 0 and y_dev.size != 0):
  dev_path = data_path / "dev" / error_path
  dev_path.mkdir(parents=True, exist_ok=True)
  # Save dev data
  print("[INFO] Saving development set data...")
  save_alignment_data(X_dev, y_dev, dev_path, mutation_path_dict, add_rand_string_to_filename)</code></pre>
  <p class="blog-post_paragraph">
    I'll then simulate a couple of alignments to test if the function is working
    correctly.
  </p>
  <pre class="line-numbers"><code class="language-python"># Create data sets
create_dir_structure_for_alignments(num_alignments=20,
                                    p_sequencing_error=0.0,
                                    p_alignment_error=0.0,
                                    remove_all_existing_data=True,
                                    set_rand_seed_data_split=(True, RANDOM_SEED),
                                    set_rand_seed_alignments=(True, RANDOM_SEED))

create_dir_structure_for_alignments(num_alignments=10,
                                    p_sequencing_error=0.1,
                                    p_alignment_error=0.05,
                                    remove_all_existing_data=False,
                                    set_rand_seed_data_split=(True, RANDOM_SEED),
                                    set_rand_seed_alignments=(True, RANDOM_SEED))

create_dir_structure_for_alignments(num_alignments=10,
                                    p_sequencing_error=0.1,
                                    p_alignment_error=0.05,
                                    set_dev_set=(True, 0.1, "split-test"),
                                    remove_all_existing_data=False,
                                    set_rand_seed_data_split=(True, RANDOM_SEED),
                                    set_rand_seed_alignments=(True, RANDOM_SEED))s</code></pre>
  <p class="blog-post_paragraph">
    Now I'll check if function organized the alignments in the correct directory
    structure.
  </p>
  <pre
    class="command-line"
    data-user=""
    data-host=""
    style="padding-top: 2.2em"
  ><code class="language-commandline">tree data</code></pre>
  <pre class="output no-lang-display"><code class="language-output">data
├── dev
│   └── 10-percent-seq-error_5-percent-alignment-error
│       └── no-mutation
│           └── 1.png
├── test
│   ├── 0-percent-seq-error_0-percent-alignment-error
│   │   ├── heterozygous-mutation
│   │   │   ├── 1.png
│   │   │   ├── 2.png
│   │   │   └── 4.png
│   │   └── no-mutation
│   │       └── 3.png
│   └── 10-percent-seq-error_5-percent-alignment-error
│       ├── heterozygous-mutation
│       │   ├── 1.png
│       │   └── 2.png
│       └── no-mutation
│           └── 1.png
└── train
    ├── 0-percent-seq-error_0-percent-alignment-error
    │   ├── heterozygous-mutation
    │   │   ├── 04.png
    │   │   ├── 05.png
    │   │   ├── 07.png
    │   │   ├── 09.png
    │   │   ├── 10.png
    │   │   ├── 13.png
    │   │   └── 16.png
    │   ├── homozygous-mutation
    │   │   ├── 02.png
    │   │   ├── 03.png
    │   │   ├── 06.png
    │   │   ├── 08.png
    │   │   ├── 11.png
    │   │   └── 15.png
    │   └── no-mutation
    │       ├── 01.png
    │       ├── 12.png
    │       └── 14.png
    └── 10-percent-seq-error_5-percent-alignment-error
        ├── heterozygous-mutation
        │   ├── 4.png
        │   └── 7.png
        ├── homozygous-mutation
        │   ├── 1.png
        │   └── 3.png
        └── no-mutation
            ├── 2.png
            ├── 5.png
            ├── 6.png
            └── 8.png

19 directories, 32 files</code></pre>
  <p class="blog-post_paragraph">
    So it looks like this function is capable of simulated various kinds of DNA
    alignment data and organizing it into the desired directory structure. It's
    even capable of generating a development set (aka a cross-validation set).
    Development sets are useful because this subset of the dataset can be used
    to evaluate a model's performance during training. It helps in tuning
    hyperparameters and assessing how well the model generalizes to new, unseen
    data. By having a separate validation set, the model's performance can be
    assessed without biasing hyperparameter tuning decisions based on the test
    set's results. This helps prevent overfitting and ensures that the model
    generalizes well to unseen data. For the purposes of this project, I won't
    utilize a development set and will just go with a train/test split to speed
    up the testing of the models but I've made the function as robust and
    generalizable as possible for future use.
  </p>
  <p class="blog-post_paragraph">
    I also can check how many files there are in each folder using the following
    function
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Inspect the data structure
def walk_through_dir(dir_path):
  """Walks through dir_path returning file counts of its contents."""
  for dirpath, dirnames, filenames in os.walk(dir_path):
    print(f"There are {len(dirnames)} directories and {len(filenames)} images in '{dirpath}'")

walk_through_dir("data/")</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">There are 3 directories and 0 images in 'data/'
There are 2 directories and 0 images in 'data/train'
There are 3 directories and 0 images in 'data/train/10-percent-seq-error_5-percent-alignment-error'
There are 0 directories and 2 images in 'data/train/10-percent-seq-error_5-percent-alignment-error/heterozygous-mutation'
There are 0 directories and 4 images in 'data/train/10-percent-seq-error_5-percent-alignment-error/no-mutation'
There are 0 directories and 2 images in 'data/train/10-percent-seq-error_5-percent-alignment-error/homozygous-mutation'
There are 3 directories and 0 images in 'data/train/0-percent-seq-error_0-percent-alignment-error'
There are 0 directories and 7 images in 'data/train/0-percent-seq-error_0-percent-alignment-error/heterozygous-mutation'
There are 0 directories and 3 images in 'data/train/0-percent-seq-error_0-percent-alignment-error/no-mutation'
There are 0 directories and 6 images in 'data/train/0-percent-seq-error_0-percent-alignment-error/homozygous-mutation'
There are 2 directories and 0 images in 'data/test'
There are 2 directories and 0 images in 'data/test/10-percent-seq-error_5-percent-alignment-error'
There are 0 directories and 2 images in 'data/test/10-percent-seq-error_5-percent-alignment-error/heterozygous-mutation'
There are 0 directories and 1 images in 'data/test/10-percent-seq-error_5-percent-alignment-error/no-mutation'
There are 2 directories and 0 images in 'data/test/0-percent-seq-error_0-percent-alignment-error'
There are 0 directories and 3 images in 'data/test/0-percent-seq-error_0-percent-alignment-error/heterozygous-mutation'
There are 0 directories and 1 images in 'data/test/0-percent-seq-error_0-percent-alignment-error/no-mutation'
There are 1 directories and 0 images in 'data/dev'
There are 1 directories and 0 images in 'data/dev/10-percent-seq-error_5-percent-alignment-error'
There are 0 directories and 1 images in 'data/dev/10-percent-seq-error_5-percent-alignment-error/no-mutation'</code></pre>
  <p class="blog-post_paragraph">
    Now I'll quickly preview some images to validate that the alignments were
    generated and saved correctly.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Display a set of alignments for an individual with no mutation when no errors are present
file_path = './data/train/0-percent-seq-error_0-percent-alignment-error/no-mutation/14.png'
Image(filename=file_path)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/12-no-mutation.png"
      alt="Example of DNA sequencing alignments for an individual with no mutation from the subsetted data folder"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Display a set of alignments for an individual with a heterozygous mutation when no errors are present
file_path = './data/test/0-percent-seq-error_0-percent-alignment-error/heterozygous-mutation/2.png'
Image(filename=file_path)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/13-heterozygous-mutation.png"
      alt="Example of DNA sequencing alignments for an individual with a heterozygous mutation from the subsetted data folder"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Display a set of alignments for an individual with a homozygous mutation when no errors are present
file_path = './data/train/0-percent-seq-error_0-percent-alignment-error/homozygous-mutation/03.png'
Image(filename=file_path)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/14-homozygous-mutation.png"
      alt="Example of DNA sequencing alignments for an individual with a homozygous mutation from the subsetted data folder"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Display a set of alignments for an individual with no mutation when errors are present
file_path = './data/test/10-percent-seq-error_5-percent-alignment-error/no-mutation/1.png'
Image(filename=file_path)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/15-no-mutation-errored.png"
      alt="Example of DNA sequencing alignments for an individual with no mutation and errors present from the subsetted data folder"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Display a set of alignments for an individual with a heterozygous mutation when errors are present
file_path = './data/test/10-percent-seq-error_5-percent-alignment-error/heterozygous-mutation/1.png'
Image(filename=file_path)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/16-heterozygous-mutation-errored.png"
      alt="Example of DNA sequencing alignments for an individual with a heterozygous mutation and errors present from the subsetted data folder"
    />
  </div>
  <pre
    class="line-numbers"
  ><code class="language-python"># Display a set of alignments for an individual with a homozygous when errors are present
file_path = './data/train/10-percent-seq-error_5-percent-alignment-error/homozygous-mutation/3.png'
Image(filename=file_path)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/17-homozygous-mutation-errored.png"
      alt="Example of DNA sequencing alignments for an individual with a homozygous mutation and errors present from the subsetted data folder"
    />
  </div>
  <p class="blog-post_paragraph">
    All the images have saved perfectly so the function I created is perfect for
    generating data. I'll know generate a ton of data in order to give the deep
    learning models something to train and test on.
  </p>
  <p class="blog-post_paragraph">
    I'll generate an equal amount of data for individuals with no-mutation,
    heterozygous mutations, and homozygous mutations
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Create real, random datasets to begin building deep learning models
N_DATA_PER_MUTATION = 10000

# Define ranges for sequencing error and alignment error probabilities
error_probabilities = []
seq_error_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2]
alignment_error_range = [0.0, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2]

# Generate permutations
for seq_error in seq_error_range:
  for alignment_error in alignment_error_range:
    error_probabilities.append((seq_error, alignment_error))

# Generate data
remove_existing_data = True
for i in tqdm(range(len(error_probabilities))):

  seq_error = error_probabilities[i][0]
  align_error = error_probabilities[i][1]

  for j in range(len(mutation_type_names)):
  mutation_type = j
  create_dir_structure_for_alignments(num_alignments=N_DATA_PER_MUTATION,
                    set_custom_mutation=(True, mutation_type),
                    p_sequencing_error=seq_error,
                    p_alignment_error=align_error,
                    remove_all_existing_data=remove_existing_data,
                    add_rand_string_to_filename=True)
  if remove_existing_data:
    remove_existing_data = False</code></pre>
  <p class="blog-post_paragraph">
    After simulating all the alignments, the data directory look likes this.
  </p>
  <pre
    class="command-line"
    data-user=""
    data-host=""
    style="padding-top: 2.2em"
  ><code class="language-commandline">tree variant-caller-data -L 2</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">variant-caller-data
├── test
│   ├── 0-percent-seq-error_0-percent-alignment-error
│   ├── 0-percent-seq-error_10-percent-alignment-error
│   ├── 0-percent-seq-error_15-percent-alignment-error
│   ├── 0-percent-seq-error_1-percent-alignment-error
│   ├── 0-percent-seq-error_20-percent-alignment-error
│   ├── 0-percent-seq-error_2-percent-alignment-error
│   ├── 0-percent-seq-error_3-percent-alignment-error
│   ├── 0-percent-seq-error_4-percent-alignment-error
│   ├── 0-percent-seq-error_5-percent-alignment-error
│   ├── 10-percent-seq-error_0-percent-alignment-error
│   ├── 10-percent-seq-error_10-percent-alignment-error
│   ├── 10-percent-seq-error_15-percent-alignment-error
│   ├── 10-percent-seq-error_1-percent-alignment-error
│   ├── 10-percent-seq-error_20-percent-alignment-error
│   ├── 10-percent-seq-error_2-percent-alignment-error
│   ├── 10-percent-seq-error_3-percent-alignment-error
│   ├── 10-percent-seq-error_4-percent-alignment-error
│   ├── 10-percent-seq-error_5-percent-alignment-error
│   ├── 15-percent-seq-error_0-percent-alignment-error
│   ├── 15-percent-seq-error_10-percent-alignment-error
│   ├── 15-percent-seq-error_15-percent-alignment-error
│   ├── 15-percent-seq-error_1-percent-alignment-error
│   ├── 15-percent-seq-error_20-percent-alignment-error
│   ├── 15-percent-seq-error_2-percent-alignment-error
│   ├── 15-percent-seq-error_3-percent-alignment-error
│   ├── 15-percent-seq-error_4-percent-alignment-error
│   ├── 15-percent-seq-error_5-percent-alignment-error
│   ├── 1-percent-seq-error_0-percent-alignment-error
│   ├── 1-percent-seq-error_10-percent-alignment-error
│   ├── 1-percent-seq-error_15-percent-alignment-error
│   ├── 1-percent-seq-error_1-percent-alignment-error
│   ├── 1-percent-seq-error_20-percent-alignment-error
│   ├── 1-percent-seq-error_2-percent-alignment-error
│   ├── 1-percent-seq-error_3-percent-alignment-error
│   ├── 1-percent-seq-error_4-percent-alignment-error
│   ├── 1-percent-seq-error_5-percent-alignment-error
│   ├── 20-percent-seq-error_0-percent-alignment-error
│   ├── 20-percent-seq-error_10-percent-alignment-error
│   ├── 20-percent-seq-error_15-percent-alignment-error
│   ├── 20-percent-seq-error_1-percent-alignment-error
│   ├── 20-percent-seq-error_20-percent-alignment-error
│   ├── 20-percent-seq-error_2-percent-alignment-error
│   ├── 20-percent-seq-error_3-percent-alignment-error
│   ├── 20-percent-seq-error_4-percent-alignment-error
│   ├── 20-percent-seq-error_5-percent-alignment-error
│   ├── 2-percent-seq-error_0-percent-alignment-error
│   ├── 2-percent-seq-error_10-percent-alignment-error
│   ├── 2-percent-seq-error_15-percent-alignment-error
│   ├── 2-percent-seq-error_1-percent-alignment-error
│   ├── 2-percent-seq-error_20-percent-alignment-error
│   ├── 2-percent-seq-error_2-percent-alignment-error
│   ├── 2-percent-seq-error_3-percent-alignment-error
│   ├── 2-percent-seq-error_4-percent-alignment-error
│   ├── 2-percent-seq-error_5-percent-alignment-error
│   ├── 3-percent-seq-error_0-percent-alignment-error
│   ├── 3-percent-seq-error_10-percent-alignment-error
│   ├── 3-percent-seq-error_15-percent-alignment-error
│   ├── 3-percent-seq-error_1-percent-alignment-error
│   ├── 3-percent-seq-error_20-percent-alignment-error
│   ├── 3-percent-seq-error_2-percent-alignment-error
│   ├── 3-percent-seq-error_3-percent-alignment-error
│   ├── 3-percent-seq-error_4-percent-alignment-error
│   ├── 3-percent-seq-error_5-percent-alignment-error
│   ├── 4-percent-seq-error_0-percent-alignment-error
│   ├── 4-percent-seq-error_10-percent-alignment-error
│   ├── 4-percent-seq-error_15-percent-alignment-error
│   ├── 4-percent-seq-error_1-percent-alignment-error
│   ├── 4-percent-seq-error_20-percent-alignment-error
│   ├── 4-percent-seq-error_2-percent-alignment-error
│   ├── 4-percent-seq-error_3-percent-alignment-error
│   ├── 4-percent-seq-error_4-percent-alignment-error
│   ├── 4-percent-seq-error_5-percent-alignment-error
│   ├── 5-percent-seq-error_0-percent-alignment-error
│   ├── 5-percent-seq-error_10-percent-alignment-error
│   ├── 5-percent-seq-error_15-percent-alignment-error
│   ├── 5-percent-seq-error_1-percent-alignment-error
│   ├── 5-percent-seq-error_20-percent-alignment-error
│   ├── 5-percent-seq-error_2-percent-alignment-error
│   ├── 5-percent-seq-error_3-percent-alignment-error
│   ├── 5-percent-seq-error_4-percent-alignment-error
│   └── 5-percent-seq-error_5-percent-alignment-error
└── train
    ├── 0-percent-seq-error_0-percent-alignment-error
    ├── 0-percent-seq-error_10-percent-alignment-error
    ├── 0-percent-seq-error_15-percent-alignment-error
    ├── 0-percent-seq-error_1-percent-alignment-error
    ├── 0-percent-seq-error_20-percent-alignment-error
    ├── 0-percent-seq-error_2-percent-alignment-error
    ├── 0-percent-seq-error_3-percent-alignment-error
    ├── 0-percent-seq-error_4-percent-alignment-error
    ├── 0-percent-seq-error_5-percent-alignment-error
    ├── 10-percent-seq-error_0-percent-alignment-error
    ├── 10-percent-seq-error_10-percent-alignment-error
    ├── 10-percent-seq-error_15-percent-alignment-error
    ├── 10-percent-seq-error_1-percent-alignment-error
    ├── 10-percent-seq-error_20-percent-alignment-error
    ├── 10-percent-seq-error_2-percent-alignment-error
    ├── 10-percent-seq-error_3-percent-alignment-error
    ├── 10-percent-seq-error_4-percent-alignment-error
    ├── 10-percent-seq-error_5-percent-alignment-error
    ├── 15-percent-seq-error_0-percent-alignment-error
    ├── 15-percent-seq-error_10-percent-alignment-error
    ├── 15-percent-seq-error_15-percent-alignment-error
    ├── 15-percent-seq-error_1-percent-alignment-error
    ├── 15-percent-seq-error_20-percent-alignment-error
    ├── 15-percent-seq-error_2-percent-alignment-error
    ├── 15-percent-seq-error_3-percent-alignment-error
    ├── 15-percent-seq-error_4-percent-alignment-error
    ├── 15-percent-seq-error_5-percent-alignment-error
    ├── 1-percent-seq-error_0-percent-alignment-error
    ├── 1-percent-seq-error_10-percent-alignment-error
    ├── 1-percent-seq-error_15-percent-alignment-error
    ├── 1-percent-seq-error_1-percent-alignment-error
    ├── 1-percent-seq-error_20-percent-alignment-error
    ├── 1-percent-seq-error_2-percent-alignment-error
    ├── 1-percent-seq-error_3-percent-alignment-error
    ├── 1-percent-seq-error_4-percent-alignment-error
    ├── 1-percent-seq-error_5-percent-alignment-error
    ├── 20-percent-seq-error_0-percent-alignment-error
    ├── 20-percent-seq-error_10-percent-alignment-error
    ├── 20-percent-seq-error_15-percent-alignment-error
    ├── 20-percent-seq-error_1-percent-alignment-error
    ├── 20-percent-seq-error_20-percent-alignment-error
    ├── 20-percent-seq-error_2-percent-alignment-error
    ├── 20-percent-seq-error_3-percent-alignment-error
    ├── 20-percent-seq-error_4-percent-alignment-error
    ├── 20-percent-seq-error_5-percent-alignment-error
    ├── 2-percent-seq-error_0-percent-alignment-error
    ├── 2-percent-seq-error_10-percent-alignment-error
    ├── 2-percent-seq-error_15-percent-alignment-error
    ├── 2-percent-seq-error_1-percent-alignment-error
    ├── 2-percent-seq-error_20-percent-alignment-error
    ├── 2-percent-seq-error_2-percent-alignment-error
    ├── 2-percent-seq-error_3-percent-alignment-error
    ├── 2-percent-seq-error_4-percent-alignment-error
    ├── 2-percent-seq-error_5-percent-alignment-error
    ├── 3-percent-seq-error_0-percent-alignment-error
    ├── 3-percent-seq-error_10-percent-alignment-error
    ├── 3-percent-seq-error_15-percent-alignment-error
    ├── 3-percent-seq-error_1-percent-alignment-error
    ├── 3-percent-seq-error_20-percent-alignment-error
    ├── 3-percent-seq-error_2-percent-alignment-error
    ├── 3-percent-seq-error_3-percent-alignment-error
    ├── 3-percent-seq-error_4-percent-alignment-error
    ├── 3-percent-seq-error_5-percent-alignment-error
    ├── 4-percent-seq-error_0-percent-alignment-error
    ├── 4-percent-seq-error_10-percent-alignment-error
    ├── 4-percent-seq-error_15-percent-alignment-error
    ├── 4-percent-seq-error_1-percent-alignment-error
    ├── 4-percent-seq-error_20-percent-alignment-error
    ├── 4-percent-seq-error_2-percent-alignment-error
    ├── 4-percent-seq-error_3-percent-alignment-error
    ├── 4-percent-seq-error_4-percent-alignment-error
    ├── 4-percent-seq-error_5-percent-alignment-error
    ├── 5-percent-seq-error_0-percent-alignment-error
    ├── 5-percent-seq-error_10-percent-alignment-error
    ├── 5-percent-seq-error_15-percent-alignment-error
    ├── 5-percent-seq-error_1-percent-alignment-error
    ├── 5-percent-seq-error_20-percent-alignment-error
    ├── 5-percent-seq-error_2-percent-alignment-error
    ├── 5-percent-seq-error_3-percent-alignment-error
    ├── 5-percent-seq-error_4-percent-alignment-error
    └── 5-percent-seq-error_5-percent-alignment-error

164 directories, 0 files</code></pre>
  <p class="blog-post_paragraph">
    The last thing I'll want to do before I can start getting the data ready
    into a machine learning compatible format is to create a function that lets
    me sample the data and copy it into a more ideal directory structure.
    Currently there the data is organized as follows
    <code
      >data -> train/test_dir -> what_errors_were_simulated ->
      what_mutation_the_alignments_show -> a_lot_of_alignments_images</code
    >. The functions that prepare the data for machine learning work best when
    the labels directly succeed the <code>train</code> or
    <code>test</code> directory, as follows
    <code
      >data -> train/test_dir -> what_mutation_the_alignments_show ->
      a_lot_of_alignments_images</code
    >
    (meaning that the folder with what errors were simulated needs to be removed
    from the directory structure and all the mutation data pooled). The problem
    with just naively doing this is that if all the data is just pooled then it
    will impossible to evenly divide the data properly if I choose to subset it.
    For example. If I want to only use 10% of the data to try training my
    models, I'll want to want to grab 10% of my samples without any errors, 10%
    from my samples with 1% sequencing error, 10% of my samples with 1%
    alignment error, etc. so I have an even distribution of all samples from
    every permutation of errors I generated in the data. If I pooled all the
    data straight away, I would only be able to sample 10% of the data overall,
    but where this 10% would come from would be up to random sampling and I
    would have an uneven distribution of samples from each set of simulations.
    Machine learning models work best if the training data is a good
    representation of the testing data so taking the time to make sure the data
    is correctly organized pays off before training the model.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def sample_and_create_pytorch_compatible_dir_structure(input_dir: pathlib.PosixPath,
                                                       output_dir: str = "data/",
                                                       p_data_to_sample: float = 1.0,
                                                       sample_all_splits: Tuple[bool, List[str]] = (True, _),
                                                       sample_all_seq_errors: Tuple[bool, List[int]] = (True, _),
                                                       sample_all_align_errors: Tuple[bool, List[int]] = (True, _),
                                                       sample_all_mutations: Tuple[bool, List[str]] = (True, _),
                                                       set_rand_seed: Tuple[bool, int] = (False, _)
                                                       ) -> None:
  """
  Sample and create a PyTorch-compatible directory structure.

  Args:
    input_dir (pathlib.PosixPath): Path to the input directory.
    output_dir (str, optional): Path to the output directory. Defaults to "data/".
    p_data_to_sample (float, optional): Percent of remaining data to sample after all filtering. Defaults to 1.0.
    sample_all_splits (Tuple[bool, List[str]], optional): Whether to sample all splits or specific splits.
      If True, all splits are sampled. If False, only specific splits listed in the second element of the tuple are sampled (Acceptable inputs: ["train", "test", "dev"]). Defaults to (True, _).
    sample_all_seq_errors (Tuple[bool, List[int]], optional): Whether to sample all sequence errors or specific sequence errors.
      If True, all sequence errors are sampled. If False, only specific sequence errors listed in the second element of the tuple are sampled (Acceptable inputs:  A list of integers).. Defaults to (True, _).
    sample_all_align_errors (Tuple[bool, List[int]], optional): Whether to sample all alignment errors or specific alignment errors.
      If True, all alignment errors are sampled. If False, only specific alignment errors listed in the second element of the tuple are sampled (Acceptable inputs: A list of integers).. Defaults to (True, _).
    sample_all_mutations (Tuple[bool, List[str]], optional): Whether to sample all mutations or specific mutations.
      If True, all mutations are sampled. If False, only specific mutations listed in the second element of the tuple are sampled (Acceptable inputs: ["no-mutation", "homozygous-mutation", "heterozygous-mutation"]).. Defaults to (True, _).
    set_rand_seed (Tuple[bool, int], optional): Whether to set a random seed and the value of the random seed. Defaults to (False, _)..

  Raises:
    FileNotFoundError: If a directory specified in the parameters is not found.
  """
  # Set the random seed if specified
  if set_rand_seed[0]:
    random.seed(set_rand_seed[1])

  # Remove the output directory if it already exists
  output_path = pathlib.Path(output_dir)
  if output_path.exists():
    print(f"[INFO] Removing previous data from {output_path}")
    shutil.rmtree(output_path)

  # Create output directory
  print(f"[INFO] Creating {output_path}")
  output_path.mkdir(parents=True, exist_ok=True)

  # Figure out whether to sample train, test, and/or dev set
  data_split_dirs_to_sample = None
  if sample_all_splits[0]:
    data_split_dirs_to_sample = list(input_dir.iterdir())
  elif not sample_all_splits[0]:
    data_split_dirs_to_sample = [pathlib.Path(input_dir / dir) for dir in sample_all_splits[1]]

  # Iterate through train, test, and/or dev directories
  for data_split_dir in tqdm(data_split_dirs_to_sample):

    # Guard clause
    if not data_split_dir.is_dir():
      raise FileNotFoundError(f"Directory '{data_split_dir}' not found.")

    # Get directory names of the error directories
    posix_paths_stems_to_str = [dir.stem for dir in data_split_dir.iterdir()]

    # Figure out whether to sample train, test, and/or dev set
    error_dirs_to_sample = None
    if sample_all_seq_errors[0] and sample_all_align_errors[0]: # Sample from every directory (default)
      error_dirs_to_sample = list(data_split_dir.iterdir())
    elif not sample_all_seq_errors[0] and sample_all_align_errors[0]: # Sample every alignment error but only some sequencing error directories
      error_dirs_to_sample = [pathlib.Path(data_split_dir / dir) for dir in posix_paths_stems_to_str if int(re.search(r'(\d+)-', dir).group(1)) in sample_all_seq_errors[1]]
    elif sample_all_seq_errors[0] and not sample_all_align_errors[0]: # Sample every sequencing error but only some alignment error directories
      error_dirs_to_sample = [pathlib.Path(data_split_dir / dir) for dir in posix_paths_stems_to_str if int(re.search(r'_(\d+)-', dir).group(1)) in sample_all_align_errors[1]]
    else: # Sample only certain directories
      error_dirs_to_sample = [pathlib.Path(data_split_dir / dir) for dir in posix_paths_stems_to_str if (int(re.search(r'(\d+)-', dir).group(1)) in sample_all_seq_errors[1]) and (int(re.search(r'_(\d+)-', dir).group(1)) in sample_all_align_errors[1])]

    for error_dir in tqdm(error_dirs_to_sample):

      # Guard clause
      if not error_dir.is_dir():
        raise FileNotFoundError(f"Directory '{error_dir}' not found.")

      # Figure out whether to sample no-mutation, heterozygous-mutation, and/or homozygous-mutation directories
      mutation_dirs_to_sample = None
      if sample_all_mutations[0]:
        mutation_dirs_to_sample = list(error_dir.iterdir())
      elif not sample_all_mutations[0]:
        mutation_dirs_to_sample = [pathlib.Path(error_dir / dir) for dir in sample_all_mutations[1]]
      for mutation_dir in mutation_dirs_to_sample:
        if mutation_dir.is_dir():

          # Sample the data
          mutation_name = mutation_dir.name
          num_data_to_sample = int(p_data_to_sample * len(list(mutation_dir.glob("*"))))
          sampled_data = random.sample(list(mutation_dir.glob("*")), num_data_to_sample)

          # Create output directory structure
          output_data_split_dir = output_path / data_split_dir.name
          output_mutation_dir = output_data_split_dir / mutation_name
          output_mutation_dir.mkdir(parents=True, exist_ok=True)

          # Copy sampled data to output directory
          for data_file in sampled_data:
            shutil.copy(data_file, output_mutation_dir)

# input_dir = Path("/content/variant-caller-data")
input_dir = Path("./variant-caller-data")
sample_and_create_pytorch_compatible_dir_structure(input_dir,
                                                   p_data_to_sample = 0.05,
                                                   sample_all_splits = (True, ),
                                                   sample_all_seq_errors = (True, ),
                                                   sample_all_align_errors = (True, ),
                                                   sample_all_mutations = (True, ),
                                                   set_rand_seed=(True, 42))</code></pre>
  <p class="blog-post_paragraph">
    I can use the following code to verify that the outputted directory
    structure is how I wanted.
  </p>
  <pre
    class="command-line"
    data-user=""
    data-host=""
    style="padding-top: 2.2em"
  ><code class="language-commandline">tree data -L 2</code></pre>
  <pre class="output no-lang-display"><code class="language-output">data
├── test
│   ├── heterozygous-mutation
│   ├── homozygous-mutation
│   └── no-mutation
└── train
    ├── heterozygous-mutation
    ├── homozygous-mutation
    └── no-mutation

8 directories, 0 files</code></pre>
  <p class="blog-post_paragraph">
    And I can use this code to verify that the subsetting of this directory
    worked.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Verify that the subsetting of this function worked
def count_files_in_train_and_test_dirs(directory: str) -> None:
  root, dirs, _ = next(os.walk(directory))
  for d in dirs:
    subdir = os.path.join(root, d)
    file_count = sum([len(files) for _, _, files in os.walk(subdir)])
    print(f'{root}{d}: {file_count} files')

# Check how many files were in the original directory
count_files_in_train_and_test_dirs("variant-caller-data/")

# Check how many files were in the new, subsetted directory
count_files_in_train_and_test_dirs("data/")</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">variant-caller-data/train: 216000 files
variant-caller-data/test: 54000 files
data/train: 10800 files
data/test: 2700 files</code></pre>
  <p class="blog-post_paragraph">
    Everything looks like it works. I requested to sample 0.5% of the total
    data. In my training data I have 10800 alignments which is 0.5% of 216000
    and in my testing data I have 2700 alignments which is 0.5% of 54000.
  </p>
  <p class="blog-post_paragraph">
    I'll continue working with the small amount of sampled data to quickly build
    and iterate my models. Once I have them up and running I'll revist sampling
    my data when I figure out how much data I want to feed them.
  </p>
  <p class="blog-post_paragraph">
    Now that images are downloaded and organized, need to make them compatible
    with PyTorch.
    <a href="https://en.wikipedia.org/wiki/PyTorch" target="_blank">PyTorch</a>
    is a popular open-source library for machine learning and deep learning
    tasks. It provides tools and functionalities to build and train neural
    networks efficiently. The fundamental data structure in PyTorch is the
    tensor.
    <a href="https://en.wikipedia.org/wiki/Tensor" target="_blank">Tensors</a>
    are multi-dimensional arrays or matrices that generalize scalars, vectors,
    and matrices to higher dimensions. In order to make the images trainable by
    a machine learning model, I need to perform the following steps:
  </p>
  <ol>
    <li>Transforming the image data into tensors</li>
    <li>
      Turning the tensor data into a <code>torch.utils.data.Dataset</code> and
      later a <code>torch.utils.data.DataLoader</code>
    </li>
  </ol>
  <p class="blog-post_paragraph">Starting with the first objective...</p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Define a custom lambda function to discard alpha channel
def remove_alpha_channel(image):
    if image.mode == 'RGBA':
        # Discard the alpha channel
        image = image.convert('RGB')
    return image

# Write a transform for turning images into tensors
data_transform = transforms.Compose([
    transforms.Lambda(remove_alpha_channel),
    transforms.ToTensor()  # converts pixel values from 0-255 to be between 0-1
])</code></pre>
  <p class="blog-post_paragraph">
    The above code stores the transformations we want applied to an image, in
    this case simply being to remove the
    <a
      href="https://manifold.net/doc/mfd8/images_and_channels.htm"
      target="_blank"
      >alpha channels</a
    >
    and to convert it to a tensor.
  </p>
  <p class="blog-post_paragraph">
    To see if the code worked, we can create a function that shows what the
    images look like before and after the transformations.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def plot_transformed_images(image_paths, transform, n = 3, padding = False, seed = 7):
  """Plots a series of random image images from image_paths."""
  random.seed(seed)
  random_image_paths = random.sample(image_paths, k = n)
  for image_path in random_image_paths:
    with PILImage.open(image_path) as f:
      fig, ax = plt.subplots(nrows=1, ncols = 2)
      ax[0].imshow(f)
      ax[0].set_title(f"Original \nsize: ({f.size[1]}, {f.size[0]})")
      ax[0].axis("off")

      # Transform and plot image
      transformed_image = transform(f)
      transformed_image_label = transform(f)
      if padding:
        # Pad transformed_image with black pixels to match dimensions of f
        h_diff = f.size[1] - transformed_image.shape[1]
        w_diff = f.size[0] - transformed_image.shape[2]
        padding_factor = 9
        pad_top = 0
        pad_bottom = 0
        pad_left = w_diff // padding_factor
        pad_right = w_diff // padding_factor
        transformed_image = torch.nn.functional.pad(transformed_image, (pad_left, pad_right, pad_top, pad_bottom), value=0)
      transformed_image = transformed_image.permute(1, 2, 0) # permute the image and make sure it's compatible with matplotlib
      ax[1].imshow(transformed_image)
      ax[1].set_title(f"Transformed \nsize: {transformed_image_label.shape}")
      ax[1].axis("off")

      fig.suptitle(f"Class: {image_path.parent.stem}", fontsize = 14, y = .8)
      
      
# Plot the images
data_path = Path("data/")
image_path_list = list(data_path.glob("**/*.png"))
plot_transformed_images(image_path_list,
                        transform = data_transform,
                        padding = True,
                        n = 3)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/18-to-tensor-1.png"
      alt="Example of DNA sequencing alignments converted to a image tensor"
    />
  </div>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/19-to-tensor-2.png"
      alt="Example of DNA sequencing alignments converted to a image tensor"
    />
  </div>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/20-to-tensor-3.png"
      alt="Example of DNA sequencing alignments converted to a image tensor"
    />
  </div>
  <p class="blog-post_paragraph">
    Plotting the images we see that the images look unchanged visually. The only
    difference is that the pixel values were arranged in tensor format. The
    image tensor has three dimensions: colour channels, height, and width. This
    is exactly what we wanted.
  </p>
  <p class="blog-post_paragraph">
    Now that the transformed images have been visualized and the transformations
    are functioning as expected, we can begin to take the next step of the
    getting the data ready to be trained by a deep learning model which is to
    generate PyTorch datasets. Datasets in PyTorch refer to python objects which
    are collections of data that are organized and structured for training and
    validating models. These datasets typically consist of input data (in this
    case images represented as tensors) and corresponding, ground truth labels
    (whether the alignment images were derived from a heterzygous, homozygous,
    or no-mutation-containing individual).
  </p>
  <p class="blog-post_paragraph">
    Since every model needs a dataset, we can create a function to create them.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def create_dataset(data_dir: pathlib.PosixPath, # target folder of images
                   transform = torchvision.transforms, # transforms to perform on data (images)
                   target_transform = None  # transforms to perform on labels (if necessary)
                   ) -> torchvision.datasets.folder.ImageFolder:
  """
  Creates a PyTorch dataset from image files in the specified directory.

  Args:
    data_dir (pathlib.PosixPath): The target folder containing the images.
    transform (torchvision.transforms): Transforms to perform on the data (images).
    target_transform (torchvision.transforms, optional): Transforms to perform on the labels (if necessary).

  Returns:
    torchvision.datasets.folder.ImageFolder: A PyTorch dataset representing the image files in the directory.

  Note:
    - This function assumes that the images are organized in folders, where each folder represents a class.
    - It uses torchvision's ImageFolder dataset class to load the images.
    - The `transform` argument can be a composition of transforms from torchvision.transforms module.
    - The `target_transform` argument can be used to apply transforms to the labels if necessary.

  Example:
    >>> dataset = create_dataset(data_dir=pathlib.Path("data/"), transform=torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()]))
  """

  data = torchvision.datasets.ImageFolder(root = data_dir,
                                          transform = transform,
                                          target_transform = target_transform)
  return data</code></pre>
  <p class="blog-post_paragraph">
    We can then create datasets for both our training and testing data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Set up train and testing paths
train_dir = data_path / "train"
test_dir = data_path / "test"

train_data = create_dataset(data_dir=train_dir,
                            transform=data_transform,
                            target_transform=None
                            )

test_data = create_dataset(data_dir=test_dir,
                            transform=data_transform,
                            target_transform=None
                            )

train_data, test_data</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">(Dataset ImageFolder
    Number of datapoints: 10800
    Root location: data/train
    StandardTransform
Transform: Compose(
               Lambda()
               ToTensor()
           ),
Dataset ImageFolder
    Number of datapoints: 2700
    Root location: data/test
    StandardTransform
Transform: Compose(
               Lambda()
               ToTensor()
           ))</code></pre>
  <p class="blog-post_paragraph">
    In order to see if the datasets were created successfully, it's always
    important to inspect your data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Get class names as a list
class_names = train_data.classes
class_names</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">['heterozygous-mutation', 'homozygous-mutation', 'no-mutation']</code></pre>
  <p class="blog-post_paragraph">
    The <code>classes</code> (or ground truth labels) of our datasets correspond
    to the three mutation profiles we're investigating. This is the importance
    of organizing your data in a properly formatted directory structure. With a
    little bit of simple code, PyTorch was able to pull the ground truth label
    names directly from the name of the parent directory the sample was stored
    in.
  </p>
  <p class="blog-post_paragraph">
    Another important attribute of the datasets to check is the
    <code>class_to_idx</code> attribute.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Also get a dict of the class names and their indexes (which are used in the labels)
class_dict = train_data.class_to_idx
class_dict</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">{'heterozygous-mutation': 0, 'homozygous-mutation': 1, 'no-mutation': 2}
  </code></pre>
  <p class="blog-post_paragraph">
    This attribute shows the "output-to-label" conversion that is made. This is
    essential because as stated, in PyTorch the fundamental unit of data is a
    tensor. In order for data to be fed into a machine learning model, it has to
    be numerically encoded and turned into a tensor. This is why we had to
    transform the images into tensors whereby each pixel value was some sort of
    numerical encoding. However this means that the data that gets spit out on
    the other end of the model also comes as a numerical tensor. An important
    part of figuring out what predictions your model is making is finding a way
    to transform the numbers it spits out into human-interpretable labels. In
    the case of our variant classifier, this means the model will return a
    <code>0</code> if it predicts that the image of alignments belonged to a
    heterozygous individual, a <code>1</code> if the individual was homozygous,
    and <code>2</code> if the individual did not have a mutation.
  </p>
  <p class="blog-post_paragraph">
    The last thing I'll check is the lenght of the datasets to make sure it
    contains the same amount of data that I sampled. I sampled 5% of 270000
    images (90000 images for each mutation profile), resulting in an estimated
    13500 images. I then split up the data so that 80% was in the training set
    and 20% was in the testing set. If the PyTorch datasets were generated
    correctly, this means that there should be 10800 and 2700 samples in these
    datasets, respectively.
  </p>
  <pre class="line-numbers"><code class="language-python"># Check the lengths
len(train_data), len(test_data)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">(10800, 2700)</code></pre>
  <p class="blog-post_paragraph">
    Perfect. The datasets were created correctly. The final step to getting the
    data ready is to put them into dataloaders. In PyTorch, dataloaders are a
    utility that help in efficiently loading and managing datasets during the
    training and validation phases of a model. They handle tasks such as data
    shuffling and parallelizing data loading, making it easier to iterate over
    the dataset during training. It also gives the benefit of "batching" the
    data, allowing you to load in the data in small batches at a time. This
    offers the benfit of training the model in a more memory efficient way
    because you don't have to load all the data into the computer's memory at
    once, but also has the added benefit of allowing your model to train a
    little before it sees the next little bit of training data, which could
    speed up the training process and allow you to find better training models
    quicker.
  </p>
  <p class="blog-post_paragraph">
    As with the datasets I'll create a function to create the dataloaders.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Turn train and test Datasets into DataLoaders
def create_dataloader(dataset: torch.utils.data.dataset.Dataset,
                      batch_size: int = 1,
                      shuffle: bool = False,
                      num_workers: int = 0
                      ) -> torch.utils.data.dataloader.DataLoader:
  """
  Creates a PyTorch data loader from the given dataset.

  Args:
    dataset (torch.utils.data.dataset.Dataset): The dataset to create the data loader from.
    batch_size (int, optional): How many samples per batch to load (default: 1).
    shuffle (bool, optional): Set to True to have the data reshuffled at every epoch (default: False).
    num_workers (int, optional): How many subprocesses to use for data loading. 0 means that the data will be loaded in the main process (default: 0).

  Returns:
    torch.utils.data.dataloader.DataLoader: A PyTorch data loader for the specified dataset.

  Note:
    - This function creates a data loader to iterate over batches of data from the given dataset.
    - The `dataset` argument should be an instance of a PyTorch dataset class (e.g., ImageFolder).
    - The `batch_size` determines the number of samples per batch.
    - Setting `shuffle` to True randomizes the order of data samples at each epoch.
    - `num_workers` controls the number of subprocesses used for data loading. It can speed up data loading by using multiple subprocesses.

  Example:
    >>> dataset = create_dataset(data_dir=pathlib.Path("data/"), transform=torchvision.transforms.Compose([torchvision.transforms.Resize((224, 224)), torchvision.transforms.ToTensor()]))
    >>> dataloader = create_dataloader(dataset, batch_size=32, shuffle=True, num_workers=4)
  """

  dataloader = torch.utils.data.dataloader.DataLoader(dataset=dataset,
                                                      batch_size=batch_size,
                                                      num_workers=num_workers,
                                                      shuffle=shuffle)
  return dataloader</code></pre>
  <p class="blog-post_paragraph">
    I'll create the dataloaders with a batch size of 32. This means all the data
    will be passed through model 32 images at a time for every round (aka epoch)
    of training. 32 was chosen as the batch size because it has been suggested
    to produce more stable and reliable training (<a
      href="https://arxiv.org/abs/1804.07612"
      target="_blank"
      >see here for more info</a
    >).
  </p>
  <pre class="line-numbers"><code class="language-python"># Check the lengths
len(train_data), len(test_data)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">(338, 85)</code></pre>
  <p class="blog-post_paragraph">
    We can see that the length of the dataloaders are 338 and 85 for the
    training and testing data respectively. This makes sense as these values
    multiplied by the batch size of 32 gives us about 10800 and 2700, the total
    amount of data in both datasets (with some remainder in the last
    dataloader). This suggests that our dataloader function is working as
    intended.
  </p>
  <p class="blog-post_paragraph">
    Let's inspect the dataloader further to get familiar with what the data
    looks like...
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Pull out a single batch from the dataloader
train_features_batch, train_labels_batch = next(iter(train_dataloader))

# Inspect the data
print(f"Image shape: {train_features_batch.shape} -> [batch_size, color_channels, height, width]")
print(f"Label shape: {train_labels_batch.shape}")</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Image shape: torch.Size([32, 3, 587, 1124]) -> [batch_size, color_channels, height, width]
Label shape: torch.Size([32])</code></pre>
  <p class="blog-post_paragraph">
    As we can see, the image data tensor now has four dimensions as opposed to
    three. These numbers correspond to the batch size of the tensor (32), the
    number of colour channels (3), the image height (587) and the image width
    (1124).
  </p>
  <p class="blog-post_paragraph">
    The last thing we can do to sanity check that our dataloader was correctly
    made is to visualize an image from one of the batches.
  </p>
  <pre class="line-numbers"><code class="language-python"># Show a sample
# Set the random seed
random_seed = 7
torch.manual_seed(random_seed)
torch.cuda.manual_seed(random_seed)

random_idx = torch.randint(0, len(train_features_batch), size=[1]).item()
img, label = train_features_batch[random_idx], train_labels_batch[random_idx]
plt.imshow(img.squeeze().permute(1, 2, 0), cmap="gray")
plt.title(class_names[label])
plt.axis("Off");
print(f"Image size: {img.shape}")
print(f"Label: {label}")</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Image size: torch.Size([3, 587, 1124])
Label: 1</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/21-sample-from-dataset.png"
      alt="Example of DNA sequencing alignments from a dataloader"
    />
  </div>
  <p class="blog-post_paragraph">
    As you can see, the image resembles the images we created in the dataset.
    This suggests that the dataloader is properly made and that we can move
    forward.
  </p>
  <p class="blog-post_paragraph">
    Now that the data is ready to be used, we can move along to creating the
    model. There are two approaches that I will use:
  </p>
  <ol>
    <li>Building a deep learning model from from scratch.</li>
    <li>Using transfer learning to re-train an existing deep learning model</li>
  </ol>
  <p class="blog-post_paragraph">
    Building a model from scratch provides complete control over its
    architecture, allowing you to tailor it specifically to the dataset and the
    problem at hand. Since you're not constrained by pre-existing architectures,
    you have the flexibility to experiment with different network architectures,
    activation functions, regularization techniques, etc. Being able to build a
    model from scratch provides a deeper understanding of deep learning concepts
    and architectures. On the other hand, transfer learning leverages
    pre-trained models that have been trained on large datasets, saving
    significant time and computational resources. Instead of training a model
    from scratch, you can fine-tune an existing model to your specific task,
    requiring less data and compute power. Pre-trained models have also learned
    generic features from vast amounts of data, which can be useful for tasks
    with limited training data. By using transfer learning, you can extract
    relevant features from the pre-trained model's intermediate layers and adapt
    them to your new task. Transfer learning has been shown to lead to
    <a href="https://www.nature.com/articles/s41467-023-41143-7" target="_blank"
      >state-of-the-art performance on many tasks</a
    >, as pre-trained models usually incorporate cutting-edge techniques and
    have been trained by experts on large amounts of data.
  </p>
  <p class="blog-post_paragraph">
    <u><em>Step 4: Building a model from scratch</em></u>
  </p>
  <p class="blog-post_paragraph">
    For the model architecture, I'll design a TinyVGG-like architecture similar
    to the one found
    <a href="https://poloclub.github.io/cnn-explainer/" target="_blank">here</a
    >. This architecture consists of a series of convolutional and pooling
    layers followed by fully connected layers. It strikes a balance between
    simplicity and effectiveness, making it suitable for many image
    classification tasks and a good starting point to build a model upon.
  </p>
  <p class="blog-post_paragraph">
    The code below shows what this model architecture looks like in PyTorch. The
    classifier block has been modified to take in our image tensor dimensions
    following the convolutions and output three predictions.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">class VariantCallerTinyVGG(nn.Module):
  def __init__(self, input_shape, hidden_units, output_shape):
    super().__init__()
    self.conv_block_1 = nn.Sequential(
      nn.Conv2d(in_channels = input_shape,
                out_channels = hidden_units,
                kernel_size = 3,
                stride = 1,
                padding = 1),
      nn.ReLU(),
      nn.Conv2d(in_channels = hidden_units,
                out_channels = hidden_units,
                kernel_size = 3,
                stride = 1,
                padding = 1),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size = 2)
    )
    self.conv_block_2 = nn.Sequential(
      nn.Conv2d(in_channels = hidden_units,
                out_channels = hidden_units,
                kernel_size = 3,
                stride = 1,
                padding = 1),
      nn.ReLU(),
      nn.Conv2d(in_channels = hidden_units,
                out_channels = hidden_units,
                kernel_size = 3,
                stride = 1,
                padding = 1),
      nn.ReLU(),
      nn.MaxPool2d(kernel_size = 2)
    )
    self.classifier = nn.Sequential(
      nn.Flatten(),
      nn.Linear(in_features = hidden_units * 146 * 281,
                out_features = output_shape)
    )

  def forward(self, x: torch.Tensor) -> torch.Tensor:
    return self.classifier(self.conv_block_2(self.conv_block_1(x)))

# Instantiate the model
model_0 = VariantCallerTinyVGG(input_shape = 3, # Number of image colour channels
                               hidden_units = 10,output_shape = len(class_names) # Number of mutation signatures
                               ).to(device)
model_0</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">VariantCallerTinyVGG(
    (conv_block_1): Sequential(
      (0): Conv2d(3, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (conv_block_2): Sequential(
      (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (1): ReLU()
      (2): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
      (3): ReLU()
      (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    )
    (classifier): Sequential(
      (0): Flatten(start_dim=1, end_dim=-1)
      (1): Linear(in_features=410260, out_features=3, bias=True)
    )
  )</code></pre>
  <p class="blog-post_paragraph">
    With the model architecture prepared, the we can move along to creating the
    transfer learning model.
  </p>
  <p class="blog-post_paragraph">
    <u><em>Step 5: Using Transfer learning</em></u>
  </p>
  <p class="blog-post_paragraph">
    Utilizing transfer learning involves re-training an existing deep learning
    model on a new dataset, leveraging the knowledge learned from pre-training
    on a large dataset. In this approach, I will employ the
    <a href="https://arxiv.org/abs/1512.03385" target="_blank">ResNet50</a>
    architecture. ResNet50 is known for its depth and efficiency in learning
    complex features from images, making it suitable for a wide range of
    image-related tasks.
  </p>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/22-resnet50.png"
      alt="ResNet50 Architecture"
    />
  </div>
  <p class="blog-post_paragraph">
    The rationale is that by leveraging the pre-trained ResNet50 model, I can
    benefit from the extensive knowledge it has on learning rich hierarchical
    representations of various objects, textures, and patterns present in
    images. This will hopefully allow it to learn complex pattern that can
    distinguish between the three mutation profiles, even in messy data.
  </p>
  <p class="blog-post_paragraph">
    The following code shows how I'll import and fine tune the ResNet50 model to
    work on the image alignment dataset.
  </p>
  <p class="blog-post_paragraph">
    First I'l need to imort the ResNet50 architecture and the most up to date
    parameter values this model learned. In PyTorch this can be simply done with
    a single line of code.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Get a set of pretrained model weights
resnet50_weights = torchvision.models.ResNet50_Weights.DEFAULT
resnet50_weights</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">ResNet50_Weights.IMAGENET1K_V2</code></pre>
  <p class="blog-post_paragraph">
    Now before I can fine-tune this model to make predictions on the mutation
    profiles, a crucial step for successfully implementing transfer learning is
    to make sure the data you're using is transformed in the same way taht the
    data the model was trained on. Pre-trained models are typically trained on
    large-scale datasets with specific preprocessing steps such as
    normalization, resizing, and data augmentation. This ensures that the input
    data is compatible with the pre-trained model's requirements but also that
    the models prediction capacity is not degraded because deep learning models
    can only make accurate predictions if the testing data is a good
    representation of the data is was trained on.
  </p>
  <p class="blog-post_paragraph">
    In PyTorch, the transformations a pre-trained model used can be extracted
    from the model weights object.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Get the transforms used to create our pretrained weights
resnet50_auto_transforms = resnet50_weights.transforms()
resnet50_auto_transforms</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">ImageClassification(
    crop_size=[224]
    resize_size=[232]
    mean=[0.485, 0.456, 0.406]
    std=[0.229, 0.224, 0.225]
    interpolation=InterpolationMode.BILINEAR
)</code></pre>
  <p class="blog-post_paragraph">
    Now that we have the data transformations used to train the ResNet50 model,
    we can re-develop the datasets and dataloaders for this model.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">resnet50_train_data = create_dataset(data_dir=train_dir,
                            transform=resnet50_auto_transforms,
                            target_transform=None
                            )

resnet50_test_data = create_dataset(data_dir=test_dir,
                            transform=resnet50_auto_transforms,
                            target_transform=None
                            )

resnet50_train_data, resnet50_test_data</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">(Dataset ImageFolder
    Number of datapoints: 10800
    Root location: data/train
    StandardTransform
Transform: ImageClassification(
               crop_size=[224]
               resize_size=[232]
               mean=[0.485, 0.456, 0.406]
               std=[0.229, 0.224, 0.225]
               interpolation=InterpolationMode.BILINEAR
           ),
Dataset ImageFolder
    Number of datapoints: 2700
    Root location: data/test
    StandardTransform
Transform: ImageClassification(
               crop_size=[224]
               resize_size=[232]
               mean=[0.485, 0.456, 0.406]
               std=[0.229, 0.224, 0.225]
               interpolation=InterpolationMode.BILINEAR
           ))</code></pre>
  <pre class="line-numbers"><code class="language-python">BATCH_SIZE = 32

resnet50_train_dataloader = create_dataloader(dataset = resnet50_train_data,
                                    batch_size = BATCH_SIZE,
                                    num_workers = os.cpu_count(),
                                    shuffle = True)

resnet50_test_dataloader = create_dataloader(dataset = resnet50_test_data,
                                    batch_size = BATCH_SIZE,
                                    num_workers = os.cpu_count(),
                                    shuffle = False)

# Check how many batches of images are in the data loaders?
len(resnet50_train_dataloader), len(resnet50_test_dataloader)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">(338, 85)</code></pre>
  <p class="blog-post_paragraph">
    Now let's visualize some of the transformed data and see how it compares to
    our original data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Manually recreate the ResNet50 transforms to make them compatible with visualzation
data_transform = transforms.Compose([
    # Resize and crop
    transforms.Resize(256),
    transforms.CenterCrop(224),
    # Apply the custom lambda function to remove alpha channel
    transforms.Lambda(remove_alpha_channel),
    # Convert to tensor
    transforms.ToTensor(),
    # Normalize
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
])

# Plot some images
plot_transformed_images(image_path_list,
                        transform = data_transform,
                        padding = True,
                        n = 3)</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/23-transforms-1.png"
      alt="Example of DNA sequencing alignments transformed with ResNet50 transforms"
    />
  </div>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/24-transforms-2.png"
      alt="Example of DNA sequencing alignments transformed with ResNet50 transforms"
    />
  </div>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/25-transforms-3.png"
      alt="Example of DNA sequencing alignments transformed with ResNet50 transforms"
    />
  </div>
  <p class="blog-post_paragraph">
    As you can see, the image to image tensor transformation had a significant
    effect on the image output this time. The image dimensions have been
    altered. In some cases, this may make the model unusable for a given use
    case, but in the case of this variant caller, this alteration shouldn't
    effect the accuracy of the model and may even improve it. The alignments
    were simulated to make sure the mutation falls directly in the middle of the
    reference DNA sequence. This means that only the middle of the image is
    important for determining what mutation signature an individual has. Since
    these transformations are all cropped in the middle of the image, the
    important information is retained following the transformation. It may also
    prevent the model from trying to find patterns in the unimportant outer
    edges of the alignment images. On the flip side, the other transformations
    such as the resizing and the recolouring operations may have introduced
    "grain" or image quality downgrades that might actually harm the models
    performance, especially in cases like differentiating between homozygous and
    non-mutation containing individuals where the differences between the groups
    can be as small as a single pixel. The only way to know how these
    transformations will affect the models accuracy is by testing it out.
  </p>
  <p class="blog-post_paragraph">
    Now that we have the model's data transformations, we can begin focusing on
    the model. Usually when performing transfer learning, you don't want to
    re-train an existing model. Instead you want to fine tune it. Often this
    means "freezing" the parameters learned in all the layers of the model
    except for the last one so that you can keep all the good parameters the
    model learned from it's previous training but leave the last few parameters
    trainable so the model can make predictions for your specific problem.
  </p>
  <p class="blog-post_paragraph">
    Let's start with freezing the model parameters.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Load the model weights into the model
resnet50_model_0 = torchvision.models.resnet50(weights=resnet50_weights).to(device)

# Freeze all base layers of the model
for param in resnet50_model_0.parameters():
    param.requires_grad = False

# Print a summary using torchinfo
summary(model=resnet50_model_0,
        input_size=(32, 3, 224, 224),
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [32, 3, 224, 224]    [32, 1000]           --                   False
├─Conv2d (conv1)                         [32, 3, 224, 224]    [32, 64, 112, 112]   (9,408)              False
├─BatchNorm2d (bn1)                      [32, 64, 112, 112]   [32, 64, 112, 112]   (128)                False
├─ReLU (relu)                            [32, 64, 112, 112]   [32, 64, 112, 112]   --                   --
├─MaxPool2d (maxpool)                    [32, 64, 112, 112]   [32, 64, 56, 56]     --                   --
├─Sequential (layer1)                    [32, 64, 56, 56]     [32, 256, 56, 56]    --                   False
│    └─Bottleneck (0)                    [32, 64, 56, 56]     [32, 256, 56, 56]    --                   False
│    │    └─Conv2d (conv1)               [32, 64, 56, 56]     [32, 64, 56, 56]     (4,096)              False
│    │    └─BatchNorm2d (bn1)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv2)               [32, 64, 56, 56]     [32, 64, 56, 56]     (36,864)             False
│    │    └─BatchNorm2d (bn2)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv3)               [32, 64, 56, 56]     [32, 256, 56, 56]    (16,384)             False
│    │    └─BatchNorm2d (bn3)            [32, 256, 56, 56]    [32, 256, 56, 56]    (512)                False
│    │    └─Sequential (downsample)      [32, 64, 56, 56]     [32, 256, 56, 56]    (16,896)             False
│    │    └─ReLU (relu)                  [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --
│    └─Bottleneck (1)                    [32, 256, 56, 56]    [32, 256, 56, 56]    --                   False
│    │    └─Conv2d (conv1)               [32, 256, 56, 56]    [32, 64, 56, 56]     (16,384)             False
│    │    └─BatchNorm2d (bn1)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv2)               [32, 64, 56, 56]     [32, 64, 56, 56]     (36,864)             False
│    │    └─BatchNorm2d (bn2)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv3)               [32, 64, 56, 56]     [32, 256, 56, 56]    (16,384)             False
│    │    └─BatchNorm2d (bn3)            [32, 256, 56, 56]    [32, 256, 56, 56]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --
│    └─Bottleneck (2)                    [32, 256, 56, 56]    [32, 256, 56, 56]    --                   False
│    │    └─Conv2d (conv1)               [32, 256, 56, 56]    [32, 64, 56, 56]     (16,384)             False
│    │    └─BatchNorm2d (bn1)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv2)               [32, 64, 56, 56]     [32, 64, 56, 56]     (36,864)             False
│    │    └─BatchNorm2d (bn2)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv3)               [32, 64, 56, 56]     [32, 256, 56, 56]    (16,384)             False
│    │    └─BatchNorm2d (bn3)            [32, 256, 56, 56]    [32, 256, 56, 56]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --
├─Sequential (layer2)                    [32, 256, 56, 56]    [32, 512, 28, 28]    --                   False
│    └─Bottleneck (0)                    [32, 256, 56, 56]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 256, 56, 56]    [32, 128, 56, 56]    (32,768)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 56, 56]    [32, 128, 56, 56]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 56, 56]    [32, 128, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 56, 56]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─Sequential (downsample)      [32, 256, 56, 56]    [32, 512, 28, 28]    (132,096)            False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
│    └─Bottleneck (1)                    [32, 512, 28, 28]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 128, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 28, 28]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
│    └─Bottleneck (2)                    [32, 512, 28, 28]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 128, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 28, 28]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
│    └─Bottleneck (3)                    [32, 512, 28, 28]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 128, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 28, 28]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
├─Sequential (layer3)                    [32, 512, 28, 28]    [32, 1024, 14, 14]   --                   False
│    └─Bottleneck (0)                    [32, 512, 28, 28]    [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 256, 28, 28]    (131,072)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 28, 28]    [32, 256, 28, 28]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 28, 28]    [32, 256, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 28, 28]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─Sequential (downsample)      [32, 512, 28, 28]    [32, 1024, 14, 14]   (526,336)            False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (1)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (2)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (3)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (4)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (5)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
├─Sequential (layer4)                    [32, 1024, 14, 14]   [32, 2048, 7, 7]     --                   False
│    └─Bottleneck (0)                    [32, 1024, 14, 14]   [32, 2048, 7, 7]     --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 512, 14, 14]    (524,288)            False
│    │    └─BatchNorm2d (bn1)            [32, 512, 14, 14]    [32, 512, 14, 14]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 14, 14]    [32, 512, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 512, 14, 14]    [32, 512, 7, 7]      (2,359,296)          False
│    │    └─BatchNorm2d (bn2)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv3)               [32, 512, 7, 7]      [32, 2048, 7, 7]     (1,048,576)          False
│    │    └─BatchNorm2d (bn3)            [32, 2048, 7, 7]     [32, 2048, 7, 7]     (4,096)              False
│    │    └─Sequential (downsample)      [32, 1024, 14, 14]   [32, 2048, 7, 7]     (2,101,248)          False
│    │    └─ReLU (relu)                  [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   --
│    └─Bottleneck (1)                    [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   False
│    │    └─Conv2d (conv1)               [32, 2048, 7, 7]     [32, 512, 7, 7]      (1,048,576)          False
│    │    └─BatchNorm2d (bn1)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv2)               [32, 512, 7, 7]      [32, 512, 7, 7]      (2,359,296)          False
│    │    └─BatchNorm2d (bn2)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv3)               [32, 512, 7, 7]      [32, 2048, 7, 7]     (1,048,576)          False
│    │    └─BatchNorm2d (bn3)            [32, 2048, 7, 7]     [32, 2048, 7, 7]     (4,096)              False
│    │    └─ReLU (relu)                  [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   --
│    └─Bottleneck (2)                    [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   False
│    │    └─Conv2d (conv1)               [32, 2048, 7, 7]     [32, 512, 7, 7]      (1,048,576)          False
│    │    └─BatchNorm2d (bn1)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv2)               [32, 512, 7, 7]      [32, 512, 7, 7]      (2,359,296)          False
│    │    └─BatchNorm2d (bn2)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv3)               [32, 512, 7, 7]      [32, 2048, 7, 7]     (1,048,576)          False
│    │    └─BatchNorm2d (bn3)            [32, 2048, 7, 7]     [32, 2048, 7, 7]     (4,096)              False
│    │    └─ReLU (relu)                  [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   --
├─AdaptiveAvgPool2d (avgpool)            [32, 2048, 7, 7]     [32, 2048, 1, 1]     --                   --
├─Linear (fc)                            [32, 2048]           [32, 1000]           (2,049,000)          False
========================================================================================================================
Total params: 25,557,032
Trainable params: 0
Non-trainable params: 25,557,032
Total mult-adds (G): 130.86
========================================================================================================================
Input size (MB): 19.27
Forward/backward pass size (MB): 5690.62
Params size (MB): 102.23
Estimated Total Size (MB): 5812.11
========================================================================================================================</code></pre>
  <p class="blog-post_paragraph">
    As you can see from this model summary, every layer in the ResNet50 model is
    now frozen. If we try to train the model, the optimizer function will not be
    able to update the model weights in any of these layers.
  </p>
  <p class="blog-post_paragraph">
    Now what's left to do is to unfreeze the last layer (the classifier layer)
    so that it can be trained. The easiest way to do this is to just overwrite
    it so that we can also change the number of outputs the model can produce at
    the same time (in our case we want it to make three predictions).
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python"># Recreate the fc layer and seed it to the target device
resnet50_model_0.fc = torch.nn.Linear(in_features=2048,
                                    out_features=len(class_names), # same number of output units as our number of classes
                                    bias=True).to(device)


# Print a summary using torchinfo
summary(model=resnet50_model_0,
        input_size=(32, 3, 224, 224),
        # col_names=["input_size", "output_size", "num_params", "trainable"],
        col_names=["input_size", "output_size", "num_params", "trainable"],
        col_width=20,
        row_settings=["var_names"])</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">========================================================================================================================
Layer (type (var_name))                  Input Shape          Output Shape         Param #              Trainable
========================================================================================================================
ResNet (ResNet)                          [32, 3, 224, 224]    [32, 3]              --                   Partial
├─Conv2d (conv1)                         [32, 3, 224, 224]    [32, 64, 112, 112]   (9,408)              False
├─BatchNorm2d (bn1)                      [32, 64, 112, 112]   [32, 64, 112, 112]   (128)                False
├─ReLU (relu)                            [32, 64, 112, 112]   [32, 64, 112, 112]   --                   --
├─MaxPool2d (maxpool)                    [32, 64, 112, 112]   [32, 64, 56, 56]     --                   --
├─Sequential (layer1)                    [32, 64, 56, 56]     [32, 256, 56, 56]    --                   False
│    └─Bottleneck (0)                    [32, 64, 56, 56]     [32, 256, 56, 56]    --                   False
│    │    └─Conv2d (conv1)               [32, 64, 56, 56]     [32, 64, 56, 56]     (4,096)              False
│    │    └─BatchNorm2d (bn1)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv2)               [32, 64, 56, 56]     [32, 64, 56, 56]     (36,864)             False
│    │    └─BatchNorm2d (bn2)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv3)               [32, 64, 56, 56]     [32, 256, 56, 56]    (16,384)             False
│    │    └─BatchNorm2d (bn3)            [32, 256, 56, 56]    [32, 256, 56, 56]    (512)                False
│    │    └─Sequential (downsample)      [32, 64, 56, 56]     [32, 256, 56, 56]    (16,896)             False
│    │    └─ReLU (relu)                  [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --
│    └─Bottleneck (1)                    [32, 256, 56, 56]    [32, 256, 56, 56]    --                   False
│    │    └─Conv2d (conv1)               [32, 256, 56, 56]    [32, 64, 56, 56]     (16,384)             False
│    │    └─BatchNorm2d (bn1)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv2)               [32, 64, 56, 56]     [32, 64, 56, 56]     (36,864)             False
│    │    └─BatchNorm2d (bn2)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv3)               [32, 64, 56, 56]     [32, 256, 56, 56]    (16,384)             False
│    │    └─BatchNorm2d (bn3)            [32, 256, 56, 56]    [32, 256, 56, 56]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --
│    └─Bottleneck (2)                    [32, 256, 56, 56]    [32, 256, 56, 56]    --                   False
│    │    └─Conv2d (conv1)               [32, 256, 56, 56]    [32, 64, 56, 56]     (16,384)             False
│    │    └─BatchNorm2d (bn1)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv2)               [32, 64, 56, 56]     [32, 64, 56, 56]     (36,864)             False
│    │    └─BatchNorm2d (bn2)            [32, 64, 56, 56]     [32, 64, 56, 56]     (128)                False
│    │    └─ReLU (relu)                  [32, 64, 56, 56]     [32, 64, 56, 56]     --                   --
│    │    └─Conv2d (conv3)               [32, 64, 56, 56]     [32, 256, 56, 56]    (16,384)             False
│    │    └─BatchNorm2d (bn3)            [32, 256, 56, 56]    [32, 256, 56, 56]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 56, 56]    [32, 256, 56, 56]    --                   --
├─Sequential (layer2)                    [32, 256, 56, 56]    [32, 512, 28, 28]    --                   False
│    └─Bottleneck (0)                    [32, 256, 56, 56]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 256, 56, 56]    [32, 128, 56, 56]    (32,768)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 56, 56]    [32, 128, 56, 56]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 56, 56]    [32, 128, 56, 56]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 56, 56]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─Sequential (downsample)      [32, 256, 56, 56]    [32, 512, 28, 28]    (132,096)            False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
│    └─Bottleneck (1)                    [32, 512, 28, 28]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 128, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 28, 28]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
│    └─Bottleneck (2)                    [32, 512, 28, 28]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 128, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 28, 28]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
│    └─Bottleneck (3)                    [32, 512, 28, 28]    [32, 512, 28, 28]    --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 128, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn1)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 128, 28, 28]    [32, 128, 28, 28]    (147,456)            False
│    │    └─BatchNorm2d (bn2)            [32, 128, 28, 28]    [32, 128, 28, 28]    (256)                False
│    │    └─ReLU (relu)                  [32, 128, 28, 28]    [32, 128, 28, 28]    --                   --
│    │    └─Conv2d (conv3)               [32, 128, 28, 28]    [32, 512, 28, 28]    (65,536)             False
│    │    └─BatchNorm2d (bn3)            [32, 512, 28, 28]    [32, 512, 28, 28]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 28, 28]    [32, 512, 28, 28]    --                   --
├─Sequential (layer3)                    [32, 512, 28, 28]    [32, 1024, 14, 14]   --                   False
│    └─Bottleneck (0)                    [32, 512, 28, 28]    [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 512, 28, 28]    [32, 256, 28, 28]    (131,072)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 28, 28]    [32, 256, 28, 28]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 28, 28]    [32, 256, 28, 28]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 28, 28]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─Sequential (downsample)      [32, 512, 28, 28]    [32, 1024, 14, 14]   (526,336)            False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (1)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (2)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (3)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (4)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
│    └─Bottleneck (5)                    [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 256, 14, 14]    (262,144)            False
│    │    └─BatchNorm2d (bn1)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 256, 14, 14]    [32, 256, 14, 14]    (589,824)            False
│    │    └─BatchNorm2d (bn2)            [32, 256, 14, 14]    [32, 256, 14, 14]    (512)                False
│    │    └─ReLU (relu)                  [32, 256, 14, 14]    [32, 256, 14, 14]    --                   --
│    │    └─Conv2d (conv3)               [32, 256, 14, 14]    [32, 1024, 14, 14]   (262,144)            False
│    │    └─BatchNorm2d (bn3)            [32, 1024, 14, 14]   [32, 1024, 14, 14]   (2,048)              False
│    │    └─ReLU (relu)                  [32, 1024, 14, 14]   [32, 1024, 14, 14]   --                   --
├─Sequential (layer4)                    [32, 1024, 14, 14]   [32, 2048, 7, 7]     --                   False
│    └─Bottleneck (0)                    [32, 1024, 14, 14]   [32, 2048, 7, 7]     --                   False
│    │    └─Conv2d (conv1)               [32, 1024, 14, 14]   [32, 512, 14, 14]    (524,288)            False
│    │    └─BatchNorm2d (bn1)            [32, 512, 14, 14]    [32, 512, 14, 14]    (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 14, 14]    [32, 512, 14, 14]    --                   --
│    │    └─Conv2d (conv2)               [32, 512, 14, 14]    [32, 512, 7, 7]      (2,359,296)          False
│    │    └─BatchNorm2d (bn2)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv3)               [32, 512, 7, 7]      [32, 2048, 7, 7]     (1,048,576)          False
│    │    └─BatchNorm2d (bn3)            [32, 2048, 7, 7]     [32, 2048, 7, 7]     (4,096)              False
│    │    └─Sequential (downsample)      [32, 1024, 14, 14]   [32, 2048, 7, 7]     (2,101,248)          False
│    │    └─ReLU (relu)                  [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   --
│    └─Bottleneck (1)                    [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   False
│    │    └─Conv2d (conv1)               [32, 2048, 7, 7]     [32, 512, 7, 7]      (1,048,576)          False
│    │    └─BatchNorm2d (bn1)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv2)               [32, 512, 7, 7]      [32, 512, 7, 7]      (2,359,296)          False
│    │    └─BatchNorm2d (bn2)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv3)               [32, 512, 7, 7]      [32, 2048, 7, 7]     (1,048,576)          False
│    │    └─BatchNorm2d (bn3)            [32, 2048, 7, 7]     [32, 2048, 7, 7]     (4,096)              False
│    │    └─ReLU (relu)                  [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   --
│    └─Bottleneck (2)                    [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   False
│    │    └─Conv2d (conv1)               [32, 2048, 7, 7]     [32, 512, 7, 7]      (1,048,576)          False
│    │    └─BatchNorm2d (bn1)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv2)               [32, 512, 7, 7]      [32, 512, 7, 7]      (2,359,296)          False
│    │    └─BatchNorm2d (bn2)            [32, 512, 7, 7]      [32, 512, 7, 7]      (1,024)              False
│    │    └─ReLU (relu)                  [32, 512, 7, 7]      [32, 512, 7, 7]      --                   --
│    │    └─Conv2d (conv3)               [32, 512, 7, 7]      [32, 2048, 7, 7]     (1,048,576)          False
│    │    └─BatchNorm2d (bn3)            [32, 2048, 7, 7]     [32, 2048, 7, 7]     (4,096)              False
│    │    └─ReLU (relu)                  [32, 2048, 7, 7]     [32, 2048, 7, 7]     --                   --
├─AdaptiveAvgPool2d (avgpool)            [32, 2048, 7, 7]     [32, 2048, 1, 1]     --                   --
├─Linear (fc)                            [32, 2048]           [32, 3]              6,147                True
========================================================================================================================
Total params: 23,514,179
Trainable params: 6,147
Non-trainable params: 23,508,032
Total mult-adds (G): 130.79
========================================================================================================================
Input size (MB): 19.27
Forward/backward pass size (MB): 5690.36
Params size (MB): 94.06
Estimated Total Size (MB): 5803.68
========================================================================================================================</code></pre>
  <p class="blog-post_paragraph">
    You can see from the summary that now the only part of the model that is
    trainable is the last layer which is our new classifier layer.
  </p>
  <p class="blog-post_paragraph">
    The last step is to functionize all this code so that multiple ResNet50
    models can be instantiated to be used for testing different parameters.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def create_resnet50_model(num_out_classes: int = 10,
                          frozen: bool = True,
                          compiled: bool = False,
                          device: str = device,
                          ) -> torch.nn.Module:
  
  """
  Create a transfer learning model based off ResNet50 architechture for classification.

  Args:
    num_out_classes (int, optional): Number of output classes. Defaults to 10.
    frozen (bool, optional): Whether to freeze the base layers. Defaults to True.
    compiled (bool, optional): Whether to compile the model. Defaults to False.
    device (str, optional): Device to load the model onto. Defaults to device.

  Returns:
    torch.nn.Module: ResNet50 model.
    torchvision.transforms: Transforms used to create pretrained weights.
  """
  
  # Get the optimal model weights
  weights = torchvision.models.ResNet50_Weights.DEFAULT

  # Get the transforms used to create our pretrained weights
  auto_transforms = weights.transforms()

  # Create the model
  resnet50_model = torchvision.models.resnet50(weights=weights).to(device)

  # Freeze all base layers of the model
  if frozen:
    for param in resnet50_model.parameters():
      param.requires_grad = False

  # Recreate the fc layer and seed it to the target device
  resnet50_model.fc = torch.nn.Linear(in_features=2048,
                                      out_features=num_out_classes,
                                      bias=True).to(device)
  
  if compiled:
    resnet50_model = torch.compile(resnet50_model).to(device)

  return resnet50_model, auto_transforms</code></pre>
  <p class="blog-post_paragraph">
    Now that both our custom and transfer learning models are ready to go, we
    can finally begin training them.
  </p>
  <p class="blog-post_paragraph">
    <u><em>Step 6: Begin training and testing models</em></u>
  </p>
  <p class="blog-post_paragraph">
    In order to begin training models, it's important create a function for
    training models. This function will be comprised of other various other
    functions to functionize different parts of the training process (training,
    testing, saving, etc.). The code for all the functions and helper functions
    is shown below.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def train_step(model: torch.nn.Module,
               dataloader: torch.utils.data.DataLoader,
               loss_fn: torch.nn.Module,
               optimizer: torch.optim.Optimizer,
               return_preds: bool = False):
  """
  Perform a single training step on the given model using the provided data.

  Parameters:
    model (torch.nn.Module): The neural network model.
    dataloader (torch.utils.data.DataLoader): DataLoader providing the training data.
    loss_fn (torch.nn.Module): Loss function to compute the loss.
    optimizer (torch.optim.Optimizer): Optimizer to update the model parameters.
    return_preds (bool): Whether to return predictions along with loss and accuracy. Default is False.

  Returns:
    Tuple[float, float]: A tuple containing the average loss and accuracy.
    Optional[Tuple[float, float, torch.Tensor]]: If return_preds is True, also returns the predicted labels.
  """

  # Put the model in train mode
  model.train()

  # Setup train loss and train accuracy values
  train_loss = 0
  train_acc = 0

  # Set up list to keep track of all labels
  all_pred_labels = []

  # Loop through data loader and data batches
  for batch, (X_train, y_train) in enumerate(dataloader):
    # Send data to target device
    X_train = X_train.to(device)
    y_train = y_train.to(device)

    # 1. Do forward pass
    y_pred = model(X_train)

    # 2. Calculate and accumulate loss
    loss_this_batch = loss_fn(y_pred, y_train)
    train_loss += loss_this_batch.item()

    # 3. Zero the gradients
    optimizer.zero_grad()

    # 4. Backpropagation
    loss_this_batch.backward()

    # 5. Perform a step of the optimizer funciton
    optimizer.step()

    # 6. Calculate and accumulate accuracy metric across all batches
    y_pred_labels = torch.argmax(torch.softmax(y_pred, dim = 1), dim = 1)
    acc_this_batch = (y_pred_labels == y_train).sum().item()/len(y_pred)
    train_acc += acc_this_batch

    # Append the labels to the preds list
    if return_preds:
      all_pred_labels.append(y_pred_labels)

  # Adjust the metrics to get average loss and average accuracy per batch
  train_loss /= len(dataloader)
  train_acc /= len(dataloader)

  # Return predictions if specified
  if return_preds:
    all_pred_labels=torch.cat(all_pred_labels).cpu()
    return train_loss, train_acc, all_pred_labels

  return train_loss, train_acc</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python">def test_step(model: torch.nn.Module,
              dataloader: torch.utils.data.DataLoader,
              loss_fn: torch.nn.Module,
              return_preds: bool = False):
  """
  Perform a single testing step on the given model using the provided data.

  Parameters:
    model (torch.nn.Module): The neural network model.
    dataloader (torch.utils.data.DataLoader): DataLoader providing the testing data.
    loss_fn (torch.nn.Module): Loss function to compute the loss.
    return_preds (bool): Whether to return predictions along with loss and accuracy. Default is False.

  Returns:
    Tuple[float, float]: A tuple containing the average loss and accuracy.
    Optional[Tuple[float, float, torch.Tensor]]: If return_preds is True, also returns the predicted labels.
  """

  # Put model in eval mode
  model.eval()

  # Set up the test loss and test accuracy values
  test_loss = 0
  test_acc = 0

  # Set up list to keep track of all labels
  all_pred_labels = []

  # Turn on inference mode context manager
  with torch.inference_mode():
    # Loop through the DataLoader batches
    for batch, (X_test, y_test) in enumerate(dataloader):
      # Send data to the target device
      X_test = X_test.to(device)
      y_test = y_test.to(device)

      # 1. Forward pass
      test_pred_logits = model(X_test)

      # 2. Calculate and accumulate loss
      loss_this_batch = loss_fn(test_pred_logits, y_test)
      test_loss += loss_this_batch.item()

      # 3. Calculate and accumulate accuracy
      test_pred_labels = torch.argmax(torch.softmax(test_pred_logits, dim = 1), dim = 1)
      acc_this_batch = (test_pred_labels == y_test).sum().item() / len(test_pred_labels)
      test_acc += acc_this_batch

      # Append the labels to the preds list
      if return_preds:
        all_pred_labels.append(test_pred_labels)

    # Adjust metrics to get an average loss and accuracy per batch
    test_loss /= len(dataloader)
    test_acc /= len(dataloader)

    # Return predictions if specified
    if return_preds:
      all_pred_labels=torch.cat(all_pred_labels).cpu()
      return test_loss, test_acc, all_pred_labels

    return test_loss, test_acc
  </code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python">def create_writer(experiment_name: str, 
          model_name: str, 
          extra: str=None) -> torch.utils.tensorboard.writer.SummaryWriter():
  """Creates a torch.utils.tensorboard.writer.SummaryWriter() instance saving to a specific log_dir.

  log_dir is a combination of runs/timestamp/experiment_name/model_name/extra.

  Where timestamp is the current date in YYYY-MM-DD_HH:MM:SS format.

  Args:
    experiment_name (str): Name of experiment.
    model_name (str): Name of model.
    extra (str, optional): Anything extra to add to the directory. Defaults to None.

  Returns:
    torch.utils.tensorboard.writer.SummaryWriter(): Instance of a writer saving to log_dir.

  Example usage:
    # Create a writer saving to "runs/2022-06-04_22:10:44/data_10_percent/effnetb2/5_epochs/"
    writer = create_writer(experiment_name="data_10_percent",
                           model_name="effnetb2",
                           extra="5_epochs")
    # The above is the same as:
    writer = SummaryWriter(log_dir="runs/2022-06-04/data_10_percent/effnetb2/5_epochs/")
  """
  import os

  # Get timestamp of current date (all experiments on certain day live in same folder)
  timestamp = datetime.now().strftime("%Y-%m-%d_%H:%M:%S") # returns current date in YYYY-MM-DD_HH:MM:SS format

  if extra:
    # Create log directory path
    log_dir = os.path.join("runs", timestamp, experiment_name, model_name, extra)
  else:
    log_dir = os.path.join("runs", timestamp, experiment_name, model_name)
    
  print(f"[INFO] Created SummaryWriter, saving to: {log_dir}...")
  return SummaryWriter(log_dir=log_dir)</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python">def save_model(model: torch.nn.Module,
               target_dir: str,
               model_name: str):
  """Saves a PyTorch model to a target directory.
  Args:
    model: A PyTorch model to save.
    target_dir: a directory for saving the model to.
    model_name: A filename for the saved model.
                Should include either ".pth" or ".pt" as the file extension.
  Example usage:
  save_model(model = model_0,
             target_dir = "models",
             model_name = "05_going_modular_tinyvgg_model.pth")
  """
  # Create target directory
  target_dir_path = Path(target_dir)
  target_dir_path.mkdir(parents = True, exist_ok = True)

  # Create model save path
  assert model_name.endswith(".pth") or model_name.endswith(".pt"),  "model_name should end with '.pt' or '.pth'"
  model_save_path = target_dir_path / model_name

  # Save the model state_dict()
  print(f"[INFO] Saving model to: {model_save_path}")
  torch.save(obj = model.state_dict(), f = model_save_path)</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python">def train(model: torch.nn.Module,
          sample_from_dir: str = "./variant-caller-data",
          output_dir: str = "data/",
          p_data_to_sample: float = 1.0,
          sample_all_splits: Tuple[bool, List[str]] = (True, _),
          sample_all_seq_errors: Tuple[bool, List[int]] = (True, _),
          sample_all_align_errors: Tuple[bool, List[int]] = (True, _),
          sample_all_mutations: Tuple[bool, List[str]] = (True, _),
          set_rand_seed: Tuple[bool, int] = (False, _),
          transform: torchvision.transforms = transforms.ToTensor(), # transforms to perform on data (images)
          target_transform: torchvision.transforms = None,  # transforms to perform on labels (if necessary)
          batch_size: int = 1,
          shuffle_train: bool = True,
          shuffle_test: bool = True,
          num_workers: int = 0,
          optimizer: torch.optim.Optimizer = torch.optim.Adam,
          learning_rate: float = 0.001,
          loss_fn: torch.nn.Module = nn.CrossEntropyLoss(),
          epochs: int = 5,
          writer: torch.utils.tensorboard.writer.SummaryWriter = None
          ) -> Dict[str, List[float]]:

  """
  Train the given model using the provided data and hyperparameters.

  Parameters:
    model (torch.nn.Module): The neural network model.
    sample_from_dir (str): Path to the directory containing the data to sample from. Default is "./variant-caller-data".
    output_dir (str): Directory to save the sampled data. Default is "data/".
    p_data_to_sample (float): Proportion of data to sample from the original dataset. Default is 1.0.
    sample_all_splits (Tuple[bool, List[str]]): Tuple indicating whether to sample all splits and which splits to sample. Default is (True, _).
    sample_all_seq_errors (Tuple[bool, List[int]]): Tuple indicating whether to sample all sequencing errors and which errors to sample. Default is (True, _).
    sample_all_align_errors (Tuple[bool, List[int]]): Tuple indicating whether to sample all alignment errors and which errors to sample. Default is (True, _).
    sample_all_mutations (Tuple[bool, List[str]]): Tuple indicating whether to sample all mutations and which mutations to sample. Default is (True, _).
    set_rand_seed (Tuple[bool, int]): Tuple indicating whether to set a random seed for sampling and the seed value. Default is (False, _).
    transform (torchvision.transforms): Transforms to perform on data (images). Default is transforms.ToTensor().
    target_transform (torchvision.transforms): Transforms to perform on labels (if necessary). Default is None.
    batch_size (int): Batch size for training and testing. Default is 1.
    shuffle_train (bool): Whether to shuffle the training data loader. Default is True.
    shuffle_test (bool): Whether to shuffle the testing data loader. Default is True.
    num_workers (int): Number of subprocesses to use for data loading. Default is 0.
    optimizer (torch.optim.Optimizer): Optimizer function to use for training. Default is torch.optim.Adam.
    learning_rate (float): Learning rate for the optimizer. Default is 0.001.
    loss_fn (torch.nn.Module): Loss function to compute the loss. Default is nn.CrossEntropyLoss().
    epochs (int): Number of epochs to train the model. Default is 5.
    writer (torch.utils.tensorboard.writer.SummaryWriter): SummaryWriter for logging training metrics. Default is None.

  Returns:
    Dict[str, List[float]]: A dictionary containing training and testing metrics.
  """

  # Start timing
  time_start = timer()

  # Sample data
  input_dir = Path(sample_from_dir)
  sample_and_create_pytorch_compatible_dir_structure(input_dir,
                                                     output_dir = output_dir,
                                                     p_data_to_sample = p_data_to_sample,
                                                     sample_all_splits = sample_all_splits,
                                                     sample_all_seq_errors = sample_all_seq_errors,
                                                     sample_all_align_errors = sample_all_align_errors,
                                                     sample_all_mutations = sample_all_mutations,
                                                     set_rand_seed = set_rand_seed)

  # Create datasets from sampled data
  print("[INFO] Creating datasets from sampled data...")
  data_path = Path(output_dir)
  train_dir = data_path / "train"
  test_dir = data_path / "test"

  train_data = create_dataset(data_dir = train_dir,
                            transform = transform,
                            target_transform = target_transform
                            )

  test_data = create_dataset(data_dir = test_dir,
                            transform = transform,
                            target_transform = target_transform
                            )

  # Create dataloaders
  print("[INFO] Creating dataloaders...")
  train_dataloader = create_dataloader(dataset = train_data,
                                      batch_size = batch_size,
                                      num_workers = num_workers,
                                      shuffle = shuffle_train)

  test_dataloader = create_dataloader(dataset = test_data,
                                      batch_size = batch_size,
                                      num_workers = num_workers,
                                      shuffle = shuffle_test)


  print("[INFO] Training the model...")
  # Create results dictionary
  results = {"train_loss": [],
             "train_acc": [],
             "train_time": [],
             "test_loss": [],
             "test_acc": [],
             "test_time": [],
             "total_time": []}

  # Create optimizer
  optimizer_fn = optimizer(model.parameters(), lr = learning_rate)

  # Loop through the training and testing steps for the number of epochs
  for epoch in tqdm(range(epochs)):
    # Train step
    train_time_start = timer()
    train_loss, train_acc = train_step(model = model,
                                       dataloader = train_dataloader,
                                       loss_fn = loss_fn,
                                       optimizer = optimizer_fn)
    train_time_end = timer()
    train_time = round(train_time_end - train_time_start, 3)
    # Test step
    test_time_start = timer()
    test_loss, test_acc = test_step(model = model,
                                    dataloader = test_dataloader,
                                    loss_fn = loss_fn)
    test_time_end = timer()
    test_time = round(test_time_end - test_time_start, 3)
    # Print out what's happening
    print(f"Epoch: {epoch + 1} | "
          f"Train Loss: {train_loss:.4f} | "
          f"Train Accuracy: {train_acc:.4f} | "
          f"Test Loss: {test_loss:.4f} | "
          f"Test Accuracy: {test_acc:.4f} "
    )

    # Updates the results dictionary
    results["train_loss"].append(train_loss)
    results["train_acc"].append(train_acc)
    results["train_time"].append(train_time)
    results["test_loss"].append(test_loss)
    results["test_acc"].append(test_acc)
    results["test_time"].append(train_time)

    # See if there's a writer, if so, log to it
    if writer:
      # Add results to SummaryWriter
      writer.add_scalars(main_tag="Loss",
                         tag_scalar_dict={"train_loss": train_loss,
                                          "test_loss": test_loss},
                         global_step=epoch)
      writer.add_scalars(main_tag="Accuracy",
                         tag_scalar_dict={"train_acc": train_acc,
                                          "test_acc": test_acc},
                         global_step=epoch)
      writer.add_scalars(main_tag="Time",
                         tag_scalar_dict={"train_time": train_time,
                                          "test_time": test_time},
                         global_step=epoch)
      # Close the writer
      writer.close()

  # End timing
  time_end = timer()
  total_time = round(time_end - time_start, 3)

  if writer:
    writer.add_scalars(main_tag="Total-time",
                         tag_scalar_dict={"total_time": total_time})
    # Close the writer
    writer.close()

  # Return the results dictionary
  return results</code></pre>
  <p class="blog-post_paragraph">
    Now that the functions are declared I'll define some common parameters to be
    used for the model training. The two most important ones are the optimizer
    and the loss function. The optimizer determines how the model's weights are
    updated during training to minimize the loss function. The
    <a href="https://arxiv.org/abs/1412.6980" target="_blank">Adam</a> optimizer
    is one of the popular optimizers used in deep learning. It adapts the
    learning rate for each parameter during training, allowing the model to
    converge faster and often with better performance compared to the other most
    common approach of stochastic gradient descent. On the other hand, the loss
    function computes the discrepancy between the predicted output of the model
    and the actual target labels. It quantifies how well the model is performing
    during training.
    <a
      href="https://www.v7labs.com/blog/cross-entropy-loss-guide"
      target="_blank"
      >Cross Entropy Loss</a
    >
    is commonly used for classification problems with multiple classes. It
    combines the softmax function, which converts raw scores into probability
    distributions, and the negative log-likelihood loss. By minimizing the loss
    function, the model learns to make more accurate predictions.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">optimizer = torch.optim.Adam
loss_fn = nn.CrossEntropyLoss()
learning_rate = 0.001
num_workers = os.cpu_count()
batch_size = 32</code></pre>
  <p class="blog-post_paragraph">
    I'll also train the models for 10
    <a href="https://www.opentrain.ai/glossary/epoch-machine-learning"
      >epochs</a
    >
    to start.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">epochs = 10</code></pre>
  <p class="blog-post_paragraph">
    I'll start by trying to build models that can classify mutation types when
    there are no errors in the data.
  </p>
  <pre class="line-numbers"><code class="language-python">p_data_to_sample = 1.0
    sample_all_seq_errors = (False, [0])
    sample_all_align_errors = (False, [0])</code></pre>
  <p class="blog-post_paragraph">
    The following code trains the variant caller built from scratch.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">experiment_name = "benchmarking"
model_name = "tinyvgg_1"
extra = "no-error-data_30000-samples"

tiny_vgg_transforms = transforms.Compose([transforms.ToTensor()])

tinyvgg_1 = VariantCallerTinyVGG(input_shape = 3,
                                  hidden_units = 10,
                                  output_shape = len(class_names)
                                  ).to(device)

writer = create_writer(experiment_name = experiment_name,
                        model_name = model_name,
                        extra = extra)

results_tinyvgg_1 = train(model = tinyvgg_1,
                          p_data_to_sample = p_data_to_sample,
                          sample_all_seq_errors = sample_all_seq_errors,
                          sample_all_align_errors = sample_all_align_errors,
                          transform = tiny_vgg_transforms,
                          optimizer = optimizer,
                          loss_fn = loss_fn,
                          learning_rate = learning_rate,
                          batch_size = batch_size,
                          num_workers = num_workers,
                          epochs = epochs,
                          writer = writer)

save_filepath = f"{experiment_name}_{model_name}_{extra}_{epochs}_epochs.pth"
save_model(model=tinyvgg_1, target_dir="models", model_name=save_filepath)
</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Epoch: 1 | Train Loss: 0.9396 | Train Accuracy: 0.5080 | Test Loss: 0.0064 | Test Accuracy: 1.0000
Epoch: 2 | Train Loss: 0.0009 | Train Accuracy: 1.0000 | Test Loss: 0.0002 | Test Accuracy: 1.0000
Epoch: 3 | Train Loss: 0.0001 | Train Accuracy: 1.0000 | Test Loss: 0.0001 | Test Accuracy: 1.0000
Epoch: 4 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000
Epoch: 5 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000
Epoch: 6 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000
Epoch: 7 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000
Epoch: 8 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000
Epoch: 9 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000
Epoch: 10 | Train Loss: 0.0000 | Train Accuracy: 1.0000 | Test Loss: 0.0000 | Test Accuracy: 1.0000</code></pre>
  <p class="blog-post_paragraph">
    To make sure the model is not overfitting or underfitting, it can be
    inightful to plot the loss and accuracy curves of the model. Overfitting
    occurs when a model learns to perform well on the training data but fails to
    generalize to unseen data. In terms of loss values, overfitting is often
    indicated by a significant gap between the training loss and the test loss.
    The training loss continues to decrease while the validation loss starts
    increasing or remains stagnant. This divergence occurs because the model
    becomes too complex and starts to memorize the training data, rather than
    learning the underlying patterns. As a result, it performs poorly on unseen
    data. Visually on a loss curve, you'll notice that the training loss curve
    continues to decrease, while the validation loss curve starts to plateau or
    even increase after a certain point. On the other hand, underfitting happens
    when a model is too simple to capture the underlying structure of the data,
    resulting in poor performance on both the training and test sets. In terms
    of loss values, underfitting is indicated by high training and test losses
    that do not decrease significantly over time. The values may show little
    improvement or even worsen as training progresses, suggesting that the model
    is not complex enough to capture the patterns in the data. Visually on a
    loss curve, both the training and test loss curves remain high and flat,
    indicating that the model is not learning from the data effectively.
  </p>
  <p class="blog-post_paragraph">The following code plots loss curves.</p>
  <pre
    class="line-numbers"
  ><code class="language-python">def tensorboard_to_df(logdir):
  """
  Convert TensorBoard event data to a pandas DataFrame.

  Args:
    logdir (str): Path to the TensorBoard log directory.

  Returns:
    pd.DataFrame: DataFrame containing the scalar data from the TensorBoard events.
  """
  
  # Load TensorBoard event accumulator
  event_acc = EventAccumulator(logdir)
  event_acc.Reload()

  # Extract tags and metrics
  tags = event_acc.Tags()['scalars']
  data = {}

  for tag in tags:
    tag_values = event_acc.Scalars(tag)
    if tag_values:
      steps = [v.step for v in tag_values]
      values = [v.value for v in tag_values]
      data[tag] = values

  # Create DataFrame
  df = pd.DataFrame(data)
  return df

def smooth_data(data, alpha=0.6):
  """
  Smooth the data using exponential moving average.

  Args:
  data (pd.Series): Data to be smoothed.
  alpha (float): Smoothing factor (default is 0.6).

  Returns:
  pd.Series: Smoothed data.
  """
  return data.ewm(alpha=alpha).mean()

def plot_loss_accuracy(train_loss_df, test_loss_df, train_acc_df, test_acc_df):
  """
  Plot training and testing loss, and training and testing accuracy as subplots.

  Args:
    train_loss_df (pd.DataFrame): DataFrame containing training loss data.
    test_loss_df (pd.DataFrame): DataFrame containing testing loss data.
    train_acc_df (pd.DataFrame): DataFrame containing training accuracy data.
    test_acc_df (pd.DataFrame): DataFrame containing testing accuracy data.
  """
  
  fig, axes = plt.subplots(2, figsize=(10, 10))

  # Plotting training and testing loss
  axes[0].plot(train_loss_df.index, smooth_data(train_loss_df['Loss']), label='Train Loss')
  axes[0].plot(test_loss_df.index, smooth_data(test_loss_df['Loss']), label='Test Loss')
  axes[0].set_xlabel('Epoch')
  axes[0].set_ylabel('Loss')
  axes[0].set_title('Training and Testing Loss')
  axes[0].legend()
  axes[0].grid(True)

  # Plotting training and testing accuracy
  axes[1].plot(train_acc_df.index, smooth_data(train_acc_df['Accuracy']), label='Train Accuracy')
  axes[1].plot(test_acc_df.index, smooth_data(test_acc_df['Accuracy']), label='Test Accuracy')
  axes[1].set_xlabel('Epoch')
  axes[1].set_ylabel('Accuracy')
  axes[1].set_title('Training and Testing Accuracy')
  axes[1].legend()
  axes[1].grid(True)

  # Set legend color to black
  for ax in axes:
    for text in ax.legend().get_texts():
      text.set_color('black')
  
  plt.subplots_adjust(hspace=0.3)
  plt.show()

# Convert TensorBoard data to pandas DataFrame
loaded_model_1_df_train_acc = tensorboard_to_df("runs/2024-04-21_17:13:05/benchmarking/tinyvgg_1/no-error-data_30000-samples/Accuracy_train_acc")
loaded_model_1_df_test_acc = tensorboard_to_df("runs/2024-04-21_17:13:05/benchmarking/tinyvgg_1/no-error-data_30000-samples/Accuracy_test_acc")
loaded_model_1_df_train_loss = tensorboard_to_df("runs/2024-04-21_17:13:05/benchmarking/tinyvgg_1/no-error-data_30000-samples/Loss_train_loss")
loaded_model_1_df_test_loss = tensorboard_to_df("runs/2024-04-21_17:13:05/benchmarking/tinyvgg_1/no-error-data_30000-samples/Loss_test_loss")

# Plotting loss and accuracy as subplots
plot_loss_accuracy(loaded_model_1_df_train_loss, loaded_model_1_df_test_loss, loaded_model_1_df_train_acc, loaded_model_1_df_test_acc)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/26-model-1-curves.png"
      alt="Loss and accuracy curves of model 1"
    />
  </div>
  <p class="blog-post_paragraph">
    Great. The loss curves show no sign of overfitting or underfitting in the
    model.
  </p>
  <p class="blog-post_paragraph">Now onto the transfer learning model.</p>
  <p class="blog-post_paragraph">
    The following code trains the variant caller built using transfer learning.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">experiment_name = "benchmarking"
model_name = "resnet50_frozen_1"
extra = "no-error-data_30000-samples"

resnet50_frozen_1, resnet50_transforms = create_resnet50_model(num_out_classes = len(class_names),
                                                              frozen = True,
                                                              device = device)

writer = create_writer(experiment_name = experiment_name, 
                       model_name = model_name, 
                       extra = extra)

results_resnet50_frozen_1 = train(model = resnet50_frozen_1,
                                  p_data_to_sample = p_data_to_sample,
                                  sample_all_seq_errors = sample_all_seq_errors,
                                  sample_all_align_errors = sample_all_align_errors,
                                  transform = resnet50_transforms,
                                  optimizer = optimizer,
                                  loss_fn = loss_fn,
                                  learning_rate = learning_rate,
                                  batch_size = batch_size,
                                  num_workers = num_workers,
                                  epochs = epochs,
                                  writer = writer)

save_filepath = f"{experiment_name}_{model_name}_{extra}_{epochs}_epochs.pth" 
save_model(model=resnet50_frozen_1, target_dir="models", model_name=save_filepath)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Epoch: 1 | Train Loss: 0.8166 | Train Accuracy: 0.5877 | Test Loss: 0.7344 | Test Accuracy: 0.6238
Epoch: 2 | Train Loss: 0.7078 | Train Accuracy: 0.6436 | Test Loss: 0.6992 | Test Accuracy: 0.6418
Epoch: 3 | Train Loss: 0.6778 | Train Accuracy: 0.6579 | Test Loss: 0.6999 | Test Accuracy: 0.6361
Epoch: 4 | Train Loss: 0.6599 | Train Accuracy: 0.6654 | Test Loss: 0.6876 | Test Accuracy: 0.6395
Epoch: 5 | Train Loss: 0.6500 | Train Accuracy: 0.6741 | Test Loss: 0.6772 | Test Accuracy: 0.6476
Epoch: 6 | Train Loss: 0.6465 | Train Accuracy: 0.6763 | Test Loss: 0.6681 | Test Accuracy: 0.6501
Epoch: 7 | Train Loss: 0.6401 | Train Accuracy: 0.6796 | Test Loss: 0.6742 | Test Accuracy: 0.6528
Epoch: 8 | Train Loss: 0.6392 | Train Accuracy: 0.6766 | Test Loss: 0.6631 | Test Accuracy: 0.6528
Epoch: 9 | Train Loss: 0.6370 | Train Accuracy: 0.6802 | Test Loss: 0.6660 | Test Accuracy: 0.6504
Epoch: 10 | Train Loss: 0.6397 | Train Accuracy: 0.6807 | Test Loss: 0.6743 | Test Accuracy: 0.6548</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python"># Convert TensorBoard data to pandas DataFrame
loaded_resnet50_model_1_df_train_acc = tensorboard_to_df("runs/2024-04-21_18:18:26/benchmarking/resnet50_frozen_1/no-error-data_30000-samples/Accuracy_train_acc")
loaded_resnet50_model_1_df_test_acc = tensorboard_to_df("runs/2024-04-21_18:18:26/benchmarking/resnet50_frozen_1/no-error-data_30000-samples/Accuracy_test_acc")
loaded_resnet50_model_1_df_train_loss = tensorboard_to_df("runs/2024-04-21_18:18:26/benchmarking/resnet50_frozen_1/no-error-data_30000-samples/Loss_train_loss")
loaded_resnet50_model_1_df_test_loss = tensorboard_to_df("runs/2024-04-21_18:18:26/benchmarking/resnet50_frozen_1/no-error-data_30000-samples/Loss_test_loss")

# Plotting loss and accuracy for ResNet-50 model
plot_loss_accuracy(loaded_resnet50_model_1_df_train_loss, loaded_resnet50_model_1_df_test_loss, loaded_resnet50_model_1_df_train_acc, loaded_resnet50_model_1_df_test_acc)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/27-model-2-curves.png"
      alt="Loss and accuracy curves of model 2"
    />
  </div>
  <p class="blog-post_paragraph">
    The loss curves for this transfer learning model remain quite high and don't
    lower quite quickly. This indicates that this model is underfitting the
    data.
  </p>
  <p class="blog-post_paragraph">
    The model built from scratch was able to accurately classify 100% of all
    data with no mutations, while the model using transfer learning was only
    able to classify ~65% of features. This indicates that perhaps a pre-trained
    model is too sophisticated a model to use when the data is relatively simple
    and in these cases less may be more.
  </p>
  <p class="blog-post_paragraph">
    The next thing I'll want to do is see how well this model generalized on
    data generated with errors. Below is a function that will run the testing
    step on data for every permutation of sequencing and alignment error.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def test_model_on_all_testing_data(model: torch.nn.Module,
                                   p_data_to_sample: float = 1.0,
                                   sample_all_seq_errors: Tuple[bool, List[int]] = (True, _),
                                   sample_all_align_errors: Tuple[bool, List[int]] = (True, _),
                                   sample_all_mutations: Tuple[bool, List[str]] = (True, _),
                                   set_rand_seed: Tuple[bool, int] = (False, _),
                                   sample_from_dir: str = "./variant-caller-data",
                                   output_dir: str = "data/",
                                   transform: torchvision.transforms = transforms.ToTensor(),
                                   target_transform: torchvision.transforms = None,
                                   batch_size: int = 1,
                                   num_workers: int = 0
                                   ) -> Dict[str, float]:
  """
  Test the model on all testing data with various error configurations.

  Parameters:
    model (torch.nn.Module): The model to be tested.
    p_data_to_sample (float): The proportion of data to sample from the input directory. Default is 1.0.
    sample_all_seq_errors (Tuple[bool, List[int]]): Tuple containing a boolean indicating whether to sample all sequencing errors and a list of specific sequencing errors to sample. Default is (True, _).
    sample_all_align_errors (Tuple[bool, List[int]]): Tuple containing a boolean indicating whether to sample all alignment errors and a list of specific alignment errors to sample. Default is (True, _).
    sample_all_mutations (Tuple[bool, List[str]]): Tuple containing a boolean indicating whether to sample all mutations and a list of specific mutations to sample. Default is (True, _).
    set_rand_seed (Tuple[bool, int]): Tuple containing a boolean indicating whether to set a random seed and the random seed value. Default is (False, _).
    sample_from_dir (str): The directory path from which to sample data. Default is "./variant-caller-data".
    output_dir (str): The directory path to save the sampled data. Default is "data/".
    transform (torchvision.transforms): Transforms to perform on data (images). Default is transforms.ToTensor().
    target_transform (torchvision.transforms): Transforms to perform on labels (if necessary). Default is None.
    batch_size (int): The batch size for testing. Default is 1.
    num_workers (int): The number of worker processes to use for data loading. Default is 0.

  Returns:
    List[Dict[str, float]]: A list of dictionaries containing the testing results for each error configuration.
  """

  model_to_test = model
  results_dicts = []
  input_dir = Path(sample_from_dir)

  seq_errors_to_test = [0, 1, 2, 3, 4, 5, 10, 15, 20]
  if not sample_all_seq_errors[0]:
    seq_errors_to_test = sample_all_seq_errors[1]

  align_errors_to_test = [0, 1, 2, 3, 4, 5, 10, 15, 20]
  if not sample_all_align_errors[0]:
    align_errors_to_test = sample_all_align_errors[1]

  for seq_error in tqdm(seq_errors_to_test):
    for align_error in align_errors_to_test:

      # Create data directory
      sample_and_create_pytorch_compatible_dir_structure(input_dir,
                                                          p_data_to_sample = p_data_to_sample,
                                                          sample_all_splits = (False, ["test"]),
                                                          sample_all_seq_errors = (False, [seq_error]),
                                                          sample_all_align_errors = (False, [align_error]),
                                                          sample_all_mutations = sample_all_mutations,
                                                          set_rand_seed = set_rand_seed)

      # Create a dataset from the directory
      print("[INFO] Creating dataset...")
      data_path = Path(output_dir)
      test_dir = data_path / "test"

      test_data = create_dataset(data_dir = test_dir,
                                 transform = transform,
                                 target_transform = target_transform
                                 )

      # Create dataloaders
      print("[INFO] Creating dataloader...")
      test_dataloader = create_dataloader(dataset = test_data,
                                          batch_size = batch_size,
                                          num_workers = num_workers,
                                          shuffle = False)


      print(f"[INFO] Testing the model on training data with {seq_error}% sequencing error and {align_error}% alignment error...")

      test_loss, test_acc, test_preds = test_step(model = model,
                                                  dataloader = test_dataloader,
                                                  loss_fn = nn.CrossEntropyLoss(),
                                                  return_preds = True)

      # Append value to the results dictionary list
      results_dict = {}
      results_dict["seq-error"] = seq_error
      results_dict["align-error"] = align_error
      results_dict["seq-align-error"] = (seq_error, align_error)
      results_dict["test-acc"] = test_acc
      results_dict["test-preds"] = test_preds
      results_dict["test-targets"] = test_data.targets
      results_dict["test-classes_to_idx"] = test_data.class_to_idx
      results_dict["test-data"] = test_data


      results_dicts.append(results_dict)

  # Regenerate full data directory to allow for further future image validation
  sample_and_create_pytorch_compatible_dir_structure(input_dir)

  return results_dicts</code></pre>
  <pre class="line-numbers"><code class="language-python"># Test model one
loaded_model_1_test_results = test_model_on_all_testing_data(model = loaded_model_1,
                                                             transform = tiny_vgg_transforms,
                                                             batch_size = batch_size,
                                                             num_workers = num_workers)
# Save model one results
with open("loaded_model_1_test_results.pkl", "wb") as f:
  pickle.dump(loaded_model_1_test_results, f)

# Test model two
loaded_resnet50_model_1_test_results = test_model_on_all_testing_data(model = loaded_resnet50_model_1,
                                                                      transform = resnet50_transforms,
                                                                      batch_size = batch_size,
                                                                      num_workers = num_workers)
# Save model two results
with open("loaded_resnet50_model_1_test_results.pkl", "wb") as f:
  pickle.dump(loaded_resnet50_model_1_test_results, f)</code></pre>
  <p class="blog-post_paragraph">
    Now that the models are tested on all the data, it'd be useful to create a
    function that can display a heatmap of the model accuracies on the data for
    every permutation of sequencing and alignment error. It would also be useful
    to see a confusion matrix of all the model predictions vs the true labels in
    the data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def plot_accuracy_heatmap_and_confusion_matrix(results_dicts, col="viridis", device='cpu'):
  """
  Plot a heatmap of accuracies and confusion matrix side by side.

  Parameters:
    results_dicts (list): List of dictionaries containing accuracy data.
    col (str): Color map for the heatmap.
    device (str): Device on which the model is trained.

  Returns:
    None
  """

  # Extract accuracy data
  seq_errors = []
  align_errors = []
  accuracies = []
  for result in results_dicts:
    seq_errors.append(result["seq-error"])
    align_errors.append(result["align-error"])
    accuracies.append(result["test-acc"])

  # Convert data to DataFrame
  df = pd.DataFrame({"Seq Error": seq_errors, "Align Error": align_errors, "Test Accuracy": accuracies})

  # Create subplot with heatmap and confusion matrix
  fig, axs = plt.subplots(2, 1, figsize=(10, 10), gridspec_kw={'height_ratios': [7, 3]})

  # Plot heatmap
  heatmap_data = df.pivot_table(index="Seq Error", columns="Align Error", values="Test Accuracy", aggfunc="mean")
  heatmap_data = heatmap_data.iloc[::-1]  # Invert rows
  heatmap_data *= 100  # Multiply values by 100
  sns.heatmap(heatmap_data, annot=True, cmap=col, fmt=".1f", ax=axs[0])
  axs[0].set_title("Accuracy Heatmap (% Correct)")
  axs[0].set_xlabel("Alignment Error (%)")
  axs[0].set_ylabel("Sequencing Error (%)")

  # Plot confusion matrix
  y_preds = torch.cat([result["test-preds"] for result in results_dicts])
  y_targets = torch.tensor([item for sublist in [result["test-targets"] for result in results_dicts] for item in sublist])
  classes_to_idx = results_dicts[0]['test-classes_to_idx']
  confmat = ConfusionMatrix(task="multiclass", num_classes=len(classes_to_idx))
  confmat_tensor = confmat(preds=y_preds.to(device), target=y_targets.to(device))

  # Plot the confusion matrix
  plot_confusion_matrix(
    conf_mat=confmat_tensor.numpy(),
    class_names=list(classes_to_idx.keys()),
    figsize = (2, 2),
    axis=axs[1]
  )
  axs[1].set_title("Confusion Matrix")

  plt.subplots_adjust(hspace=0.3)  # Adjust the gap between subplots
  plt.show()</code></pre>
  <p class="blog-post_paragraph">
    If we plot the accuracies of the better performing model (the custom built
    one), we see this.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">plot_accuracy_heatmap_and_confusion_matrix(loaded_model_1_test_results)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/28-model-1-heatmap.png"
      alt="Accuracy heatmap and confusion matrix of model 1"
    />
  </div>

  <p class="blog-post_paragraph">
    We can see that the model is able to correctly identify all mutations in the
    data at 100% accuracy when there are no sequencing and alignment errors as
    shown by the block in the bottom-left corner of the heatmap. It also shows
    that despite not being trained on any data with errors the model is still
    pretty good at identifying data when alignment errors are present, being
    able to still correctly identify 73.8% of the mutation profiles when
    alignment error was as high as 20%. However, as soon as any sequencing data
    is added to the data, the model's performance steeply drops off. At the
    highest amount of error (20% sequencing error and 20% alignment error), the
    model has 35.5% accuracy. When picking between one of three labels, this is
    about as good as random guessing.
  </p>
  <p class="blog-post_paragraph">
    From the confusion matrix you can also see that this model is okay at
    detecting heterozygous and homozygous mutations but struggles at detecting
    when there are no mutations. It seems like any sequencing error introduced
    tricks the model into thinking that there is an error present even when
    there's not.
  </p>
  <p class="blog-post_paragraph">
    In order to try and improve the model's performance, we can add errored data
    into the training data and retrain the model.
  </p>
  <p class="blog-post_paragraph">
    The following code subsets the data to include errored data and train with a
    similar amount of data as was used in the original models.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">p_data_to_sample = 0.11
sample_all_seq_errors = (True, )
sample_all_align_errors = (True, )</code></pre>
  <p class="blog-post_paragraph">
    Now to retrain the variant caller built from scratch with the new data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">experiment_name = "benchmarking"
model_name = "tinyvgg_2"
extra = "data-with-errors_30000-samples"

tinyvgg_2 = VariantCallerTinyVGG(input_shape = 3,
                  hidden_units = 10,
                  output_shape = len(class_names)
                  ).to(device)

writer = create_writer(experiment_name = experiment_name,
                       model_name = model_name,
                       extra = extra)

results_tinyvgg_2 = train(model = tinyvgg_2,
                          p_data_to_sample = p_data_to_sample,
                          sample_all_seq_errors = sample_all_seq_errors,
                          sample_all_align_errors = sample_all_align_errors,
                          transform = tiny_vgg_transforms,
                          optimizer = optimizer,
                          loss_fn = loss_fn,
                          learning_rate = learning_rate,
                          batch_size = batch_size,
                          num_workers = num_workers,
                          epochs = epochs,
                          writer = writer)

save_filepath = f"{experiment_name}_{model_name}_{extra}_{epochs}_epochs.pth"
save_model(model=tinyvgg_2, target_dir="models", model_name=save_filepath)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Epoch: 1 | Train Loss: 1.1367 | Train Accuracy: 0.3321 | Test Loss: 1.0986 | Test Accuracy: 0.3331
Epoch: 2 | Train Loss: 1.0987 | Train Accuracy: 0.3324 | Test Loss: 1.0986 | Test Accuracy: 0.3330
Epoch: 3 | Train Loss: 1.0988 | Train Accuracy: 0.3299 | Test Loss: 1.0986 | Test Accuracy: 0.3333
Epoch: 4 | Train Loss: 1.0987 | Train Accuracy: 0.3295 | Test Loss: 1.0986 | Test Accuracy: 0.3332
Epoch: 5 | Train Loss: 1.0987 | Train Accuracy: 0.3307 | Test Loss: 1.0986 | Test Accuracy: 0.3336
Epoch: 6 | Train Loss: 1.0987 | Train Accuracy: 0.3297 | Test Loss: 1.0987 | Test Accuracy: 0.3334
Epoch: 7 | Train Loss: 1.0987 | Train Accuracy: 0.3307 | Test Loss: 1.0987 | Test Accuracy: 0.3334
Epoch: 8 | Train Loss: 1.0987 | Train Accuracy: 0.3318 | Test Loss: 1.0986 | Test Accuracy: 0.3333
Epoch: 9 | Train Loss: 1.0987 | Train Accuracy: 0.3324 | Test Loss: 1.0987 | Test Accuracy: 0.3331
Epoch: 10 | Train Loss: 1.0987 | Train Accuracy: 0.3293 | Test Loss: 1.0986 | Test Accuracy: 0.3334</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python"># Convert TensorBoard data to pandas DataFrame
loaded_model_2_df_train_acc = tensorboard_to_df("runs/2024-04-21_22:04:32/benchmarking/tinyvgg_2/data-with-errors_30000-samples/Accuracy_train_acc")
loaded_model_2_df_test_acc = tensorboard_to_df("runs/2024-04-21_22:04:32/benchmarking/tinyvgg_2/data-with-errors_30000-samples/Accuracy_test_acc")
loaded_model_2_df_train_loss = tensorboard_to_df("runs/2024-04-21_22:04:32/benchmarking/tinyvgg_2/data-with-errors_30000-samples/Loss_train_loss")
loaded_model_2_df_test_loss = tensorboard_to_df("runs/2024-04-21_22:04:32/benchmarking/tinyvgg_2/data-with-errors_30000-samples/Loss_test_loss")

# Plotting loss and accuracy for ResNet-50 model
plot_loss_accuracy(loaded_model_2_df_train_loss, loaded_model_2_df_test_loss, loaded_model_2_df_train_acc, loaded_model_2_df_test_acc)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/29-model-3-curves.png"
      alt="Loss and accuracy curves of model 3"
    />
  </div>
  <p class="blog-post_paragraph">
    Once again the loss curves show that the loss values for this model are
    quite high and don't decrease. This indicates underfitting for this model.
  </p>
  <p class="blog-post_paragraph">
    I'll also retry transfer learning with this dataset to see if that will
    improve the accuracy of the model.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">experiment_name = "benchmarking"
model_name = "resnet50_frozen_2"
extra = "data-with-errors_30000-samples"

resnet50_frozen_2, resnet50_transforms = create_resnet50_model(num_out_classes = len(class_names),
                                                              frozen = True,
                                                              device = device)

writer = create_writer(experiment_name = experiment_name,
                       model_name = model_name,
                       extra = extra)

results_resnet50_frozen_2 = train(model = resnet50_frozen_2,
                                  p_data_to_sample = p_data_to_sample,
                                  sample_all_seq_errors = sample_all_seq_errors,
                                  sample_all_align_errors = sample_all_align_errors,
                                  transform = resnet50_transforms,
                                  optimizer = optimizer,
                                  loss_fn = loss_fn,
                                  learning_rate = learning_rate,
                                  batch_size = batch_size,
                                  num_workers = num_workers,
                                  epochs = epochs,
                                  writer = writer)

save_filepath = f"{experiment_name}_{model_name}_{extra}_{epochs}_epochs.pth"
save_model(model=resnet50_frozen_2, target_dir="models", model_name=save_filepath)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Epoch: 1 | Train Loss: 1.0812 | Train Accuracy: 0.3909 | Test Loss: 1.0626 | Test Accuracy: 0.4146
Epoch: 2 | Train Loss: 1.0510 | Train Accuracy: 0.4359 | Test Loss: 1.0618 | Test Accuracy: 0.4137
Epoch: 3 | Train Loss: 1.0390 | Train Accuracy: 0.4425 | Test Loss: 1.0746 | Test Accuracy: 0.4035
Epoch: 4 | Train Loss: 1.0325 | Train Accuracy: 0.4566 | Test Loss: 1.0560 | Test Accuracy: 0.4252
Epoch: 5 | Train Loss: 1.0287 | Train Accuracy: 0.4574 | Test Loss: 1.0722 | Test Accuracy: 0.4191
Epoch: 6 | Train Loss: 1.0222 | Train Accuracy: 0.4644 | Test Loss: 1.0722 | Test Accuracy: 0.4182
Epoch: 7 | Train Loss: 1.0266 | Train Accuracy: 0.4608 | Test Loss: 1.0748 | Test Accuracy: 0.4114
Epoch: 8 | Train Loss: 1.0239 | Train Accuracy: 0.4661 | Test Loss: 1.0755 | Test Accuracy: 0.4158
Epoch: 9 | Train Loss: 1.0234 | Train Accuracy: 0.4666 | Test Loss: 1.0864 | Test Accuracy: 0.4123
Epoch: 10 | Train Loss: 1.0199 | Train Accuracy: 0.4680 | Test Loss: 1.1123 | Test Accuracy: 0.4141</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python"># Convert TensorBoard data to pandas DataFrame
loaded_resnet50_model_2_df_train_acc = tensorboard_to_df("runs/2024-04-21_23:09:13/benchmarking/resnet50_frozen_2/data-with-errors_30000-samples/Accuracy_train_acc")
loaded_resnet50_model_2_df_test_acc = tensorboard_to_df("runs/2024-04-21_23:09:13/benchmarking/resnet50_frozen_2/data-with-errors_30000-samples/Accuracy_test_acc")
loaded_resnet50_model_2_df_train_loss = tensorboard_to_df("runs/2024-04-21_23:09:13/benchmarking/resnet50_frozen_2/data-with-errors_30000-samples/Loss_train_loss")
loaded_resnet50_model_2_df_test_loss = tensorboard_to_df("runs/2024-04-21_23:09:13/benchmarking/resnet50_frozen_2/data-with-errors_30000-samples/Loss_test_loss")

# Plotting loss and accuracy for ResNet-50 model
plot_loss_accuracy(loaded_resnet50_model_2_df_train_loss, loaded_resnet50_model_2_df_test_loss, loaded_resnet50_model_2_df_train_acc, loaded_resnet50_model_2_df_test_acc)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/30-model-4-curves.png"
      alt="Loss and accuracy curves of model 4"
    />
  </div>
  <p class="blog-post_paragraph">
    This model looks like it is underfitting the data as well.
  </p>
  <p class="blog-post_paragraph">
    Let's test both models to see if any improvements were made to the model
    accuracies.
  </p>
  <pre class="line-numbers"><code class="language-python"># Test model three 
loaded_model_2_test_results = test_model_on_all_testing_data(model = loaded_model_2,
                                                             transform = tiny_vgg_transforms,
                                                             batch_size = batch_size,
                                                             num_workers = num_workers)
# Save model three results
with open("loaded_model_2_test_results.pkl", "wb") as f:
  pickle.dump(loaded_model_2_test_results, f)

# Test model four
loaded_resnet50_model_2_test_results = test_model_on_all_testing_data(model = loaded_resnet50_model_2,
                                                                      transform = resnet50_transforms,
                                                                      batch_size = batch_size,
                                                                      num_workers = num_workers)
# Save model four results
with open("loaded_resnet50_model_2_test_results.pkl", "wb") as f:
  pickle.dump(loaded_resnet50_model_2_test_results, f)</code></pre>
  <p class="blog-post_paragraph">
    This time the transfer learning model seemed to do better. Let's inspect is
    robustness to errored data.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">plot_accuracy_heatmap_and_confusion_matrix(loaded_resnet50_model_2_test_results)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/31-model-4-heatmap.png"
      alt="Accuracy heatmap and confusion matrix of model 4"
    />
  </div>
  <p class="blog-post_paragraph">
    From this heatmap, it looks like the same general trend arises. The model
    performs better when alignment error is present than if sequencing error is
    present. However, the overall performance of this model is worse than the
    previous model. This may be for a couple reasons.
  </p>
  <ol>
    <li>
      The transfer learning model has most of it's layers frozen, meaning the
      model cannot update the model weights of hidden layers of the model. Since
      the model was trained on real image data, it could be the case that these
      hidden layers are trained to extract features that may be present in
      real-world images but not present in the alignemnt data. In this case, it
      may be beneficial to "unfreeze" these layers and try to let the hidden
      layers learn the features of this dataset specifically.
    </li>
    <li>
      The model may not have had enough data to train on. I subsetted my data so
      that only 10% of it was fed into the models to try and train the models
      faster. As a general rule of machine learing, more data + more compute (a
      bigger model) = higher accuracy.
    </li>
    <li>
      The model may not have been trained for long enough. I let the model train
      for 10 epochs. Usually, real life models can train for months and can
      start around 600 epochs. The risk with training for longer is that the
      models may start to overfit your training data and be less generalizable
      to the testing data however in our case this didn't seem to be a problem
      after 10 epochs.
    </li>
  </ol>
  <p class="blog-post_paragraph">
    I'll retry training the transfer learning approach with an unfrozen model,
    all of the data I simulated, and for double the training time I used before.
  </p>
  <pre class="line-numbers"><code class="language-python">p_data_to_sample = 1.0
sample_all_seq_errors = (True, )
sample_all_align_errors = (True, )
epochs = 20</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python">experiment_name = "benchmarking"
model_name = "resnet50_unfrozen_3"
extra = "data-with-errors_90000-samples"

resnet50_unfrozen_3, resnet50_transforms = create_resnet50_model(num_out_classes = len(class_names),
                                                              frozen = False,
                                                              device = device)

writer = create_writer(experiment_name = experiment_name,
                        model_name = model_name,
                        extra = extra)

results_resnet50_unfrozen_3 = train(model = resnet50_unfrozen_3,
                                    p_data_to_sample = p_data_to_sample,
                                    sample_all_seq_errors = sample_all_seq_errors,
                                    sample_all_align_errors = sample_all_align_errors,
                                    transform = resnet50_transforms,
                                    optimizer = optimizer,
                                    loss_fn = loss_fn,
                                    learning_rate = learning_rate,
                                    batch_size = batch_size,
                                    num_workers = num_workers,
                                    epochs = epochs,
                                    writer = writer)

save_filepath = f"{experiment_name}_{model_name}_{extra}_{epochs}_epochs.pth"
save_model(model=resnet50_unfrozen_3, target_dir="models", model_name=save_filepath)</code></pre>
  <pre
    class="output no-lang-display"
  ><code class="language-output">Epoch: 1 | Train Loss: 0.2732 | Train Accuracy: 0.8744 | Test Loss: 0.1259 | Test Accuracy: 0.9541
Epoch: 2 | Train Loss: 0.0897 | Train Accuracy: 0.9680 | Test Loss: 0.0783 | Test Accuracy: 0.9729
Epoch: 3 | Train Loss: 0.0644 | Train Accuracy: 0.9778 | Test Loss: 0.0645 | Test Accuracy: 0.9780
Epoch: 4 | Train Loss: 0.0532 | Train Accuracy: 0.9823 | Test Loss: 0.0526 | Test Accuracy: 0.9832
Epoch: 5 | Train Loss: 0.0460 | Train Accuracy: 0.9850 | Test Loss: 0.0489 | Test Accuracy: 0.9841
Epoch: 6 | Train Loss: 0.0411 | Train Accuracy: 0.9864 | Test Loss: 0.0511 | Test Accuracy: 0.9834
Epoch: 7 | Train Loss: 0.0370 | Train Accuracy: 0.9882 | Test Loss: 0.0464 | Test Accuracy: 0.9850
Epoch: 8 | Train Loss: 0.0340 | Train Accuracy: 0.9891 | Test Loss: 0.0436 | Test Accuracy: 0.9864
Epoch: 9 | Train Loss: 0.0311 | Train Accuracy: 0.9902 | Test Loss: 0.0457 | Test Accuracy: 0.9853
Epoch: 10 | Train Loss: 0.0284 | Train Accuracy: 0.9908 | Test Loss: 0.0455 | Test Accuracy: 0.9859
Epoch: 11 | Train Loss: 0.0254 | Train Accuracy: 0.9919 | Test Loss: 0.0485 | Test Accuracy: 0.9858
Epoch: 12 | Train Loss: 0.0225 | Train Accuracy: 0.9927 | Test Loss: 0.0464 | Test Accuracy: 0.9866
Epoch: 13 | Train Loss: 0.0202 | Train Accuracy: 0.9935 | Test Loss: 0.0531 | Test Accuracy: 0.9859
Epoch: 14 | Train Loss: 0.0183 | Train Accuracy: 0.9940 | Test Loss: 0.0529 | Test Accuracy: 0.9845
Epoch: 15 | Train Loss: 0.0163 | Train Accuracy: 0.9946 | Test Loss: 0.0501 | Test Accuracy: 0.9856
Epoch: 16 | Train Loss: 0.0141 | Train Accuracy: 0.9952 | Test Loss: 0.0552 | Test Accuracy: 0.9861
Epoch: 17 | Train Loss: 0.0128 | Train Accuracy: 0.9957 | Test Loss: 0.0629 | Test Accuracy: 0.9847
Epoch: 18 | Train Loss: 0.0122 | Train Accuracy: 0.9959 | Test Loss: 0.0575 | Test Accuracy: 0.9854
Epoch: 19 | Train Loss: 0.0101 | Train Accuracy: 0.9965 | Test Loss: 0.0706 | Test Accuracy: 0.9850
Epoch: 20 | Train Loss: 0.0095 | Train Accuracy: 0.9969 | Test Loss: 0.0692 | Test Accuracy: 0.9859</code></pre>
  <pre
    class="line-numbers"
  ><code class="language-python"># Convert TensorBoard data to pandas DataFrame
loaded_resnet50_model_3_df_train_acc = tensorboard_to_df("runs/2024-04-19_01_12_08/benchmarking/resnet50_unfrozen_3/data-with-errors_90000-samples/Accuracy_train_acc")
loaded_resnet50_model_3_df_test_acc = tensorboard_to_df("runs/2024-04-19_01_12_08/benchmarking/resnet50_unfrozen_3/data-with-errors_90000-samples/Accuracy_test_acc")
loaded_resnet50_model_3_df_train_loss = tensorboard_to_df("runs/2024-04-19_01_12_08/benchmarking/resnet50_unfrozen_3/data-with-errors_90000-samples/Loss_train_loss")
loaded_resnet50_model_3_df_test_loss = tensorboard_to_df("runs/2024-04-19_01_12_08/benchmarking/resnet50_unfrozen_3/data-with-errors_90000-samples/Loss_test_loss")

# Plotting loss and accuracy for ResNet-50 model
plot_loss_accuracy(loaded_resnet50_model_3_df_train_loss, loaded_resnet50_model_3_df_test_loss, loaded_resnet50_model_3_df_train_acc, loaded_resnet50_model_3_df_test_acc)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/32-model-5-curves.png"
      alt="Loss and accuracy curves of model 5"
    />
  </div>
  <p class="blog-post_paragraph">
    The loss curves on this model look much better. There is a slight divergence
    towards the tail end of the training time which might indicate that
    overfitting is beginning to start, but the upwards trend is very shallow and
    this was predictable due to the longer time this model was trained for.
    Overally the loss curves and accuracy look pretty good.
  </p>
  <p class="blog-post_paragraph">
    Let's inspect this model's performance further.
  </p>
  <pre class="line-numbers"><code class="language-python"># Test model five
loaded_resnet50_model_3_test_results = test_model_on_all_testing_data(model = loaded_resnet50_model_3,
                                                                      transform = resnet50_transforms,
                                                                      batch_size = batch_size,
                                                                      num_workers = num_workers)
# Save model five results
with open("loaded_resnet50_model_3_test_results.pkl", "wb") as f:
  pickle.dump(loaded_resnet50_model_3_test_results, f)

# Plot accuracy heatmap and confusion matrix for model five 
plot_accuracy_heatmap_and_confusion_matrix(loaded_resnet50_model_3_test_results)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/33-model-5-heatmap.png"
      alt="Accuracy heatmap and confusion matrix of model 5"
    />
  </div>
  <p class="blog-post_paragraph">
    Beautiful. This model had an average accuracy of 98.6%. It can accurately
    predict mutation patterns in a wide variety of sequencing and alignment
    error permutations. Even when sequencing error and alignment error are as
    high as 20% each, the model can still accurately classify 90.1% of the
    mutation patterns.
  </p>
  <p class="blog-post_paragraph">
    The last thing I'll do is to visualize some of these model predictions.
  </p>
  <pre
    class="line-numbers"
  ><code class="language-python">def visualize_samples(model_results: List[Dict],
            num_samples: int = 6,
            set_seed: Tuple[bool, int] = (False, _),
            grid_cols: int = 2,
            ):
  """
  Visualize randomly selected samples from model results in a 3x2 grid layout.

  Parameters:
    model_results (list): List of dictionaries containing model results.
    translation_dict (dict): Dictionary for translating labels.
    num_samples (int): Number of samples to visualize. Default is 6.
    set_seed: Tuple[bool, int] = (False, _)
    grid_rows (int): Number of rows in the grid. Default is 3.
  """

  num_rows = math.ceil(num_samples / grid_cols)
  plt.figure(figsize=(6, num_samples))

  for i in range(num_samples):

    # Set random seed if given
    if set_seed[0]:
      random.seed(set_seed[1])

    # get image and labels from test data
    # print(model_results)
    random_image_idx = random.randint(0, len(model_results) - 1)
    random_image_idx_2 = random.randint(0, len(model_results[random_image_idx]["test-data"]) - 1)
    # print(model_results[random_image_idx]["test-data"][random_image_idx_2])
    img = tiny_vgg_transforms(PILImage.open(model_results[random_image_idx]["test-data"].imgs[random_image_idx_2][0]))
    label = model_results[random_image_idx]["test-data"].imgs[random_image_idx_2][1]
    # print(model_results[random_image_idx]["test-data"][random_image_idx_2])
    # print(label)
    model_pred_label = model_results[random_image_idx]['test-preds'][random_image_idx_2].cpu().item()

    # Create the subplot
    plt.subplot(num_rows, grid_cols, i + 1)
    plt.imshow(img.permute(1, 2, 0))

    # Determine text color based on prediction correctness
    text_color = 'lightgreen' if label == model_pred_label else 'red'

    # Plot the figure with colored text
    translation_dict = {0: 'Heterozygous', 1: 'Homozygous', 2: 'No mutation'}
    plt.title(f"True Label: {translation_dict[label]} \n Model Prediction: {translation_dict[model_pred_label]}", color=text_color)
    plt.axis(False)

  plt.subplots_adjust(wspace=0, hspace=0)
  plt.tight_layout()

# Visualize predictions
random.seed(1)
visualize_samples(loaded_resnet50_model_3_test_results, num_samples=14)</code></pre>
  <pre
    class="output-img no-code"
  ><code class="language-output">&nbsp;</code></pre>
  <div class="workshowcase-img-container">
    <img
      class="img-fluid"
      src="assets/img/blog_posts/building-a-variant-caller/34-model-5-predictions.png"
      alt="Model predictions vs true labels on a variety of DNA alignment image samples"
    />
  </div>
  <p class="blog-post_paragraph">
    So to summarize, I was able to build a deep learning model that could
    classify whether an individual had zero, one, or two copies of a mutation
    from a set of simulated DNA alignment data with 98.6% accuracy. By
    leveraging techniques such as transfer learning, data augmentation, and
    careful model selection, I achieved great classification accuracy. This
    problem was an excellent exercise in learning how deep learning and neural
    network models work under the hood. Deep learning is becoming increasingly
    ubiquitous in various fields, including biology and genetics. For
    researchers in genetics and related fields, understanding and applying deep
    learning techniques is becoming essential for staying at the forefront of
    research. It is a powerful tool for analyzing complex datasets and
    extracting meaningful patterns that may not be immediately apparent to human
    researchers. The intersection of deep learning and genetics holds tremendous
    promise for advancing our understanding of complex biological systems and
    developing novel biotechnologies. Now that I understand how these models
    work and how they can be built I feel much more confident in my skills as a
    researcher. I am able to now comprehend the latest literature which features
    deep learning approach and be able to effectively work with teams of
    interdisciplinary people on complex data-related challenges. AI has the
    power to change the world in a beneficial way and I'm excited to see how it
    will advance all fields of science.
  </p>
</div>
